---
title: "Kaizen 1"
date: "2025-12-08"
relatedPost: "/posts/kaizen_2"
tags:
  - process
  - thread
  - server
  - continuous improvement
  - memory leak
---

# kaizen 1

별 다른 의도는 없고, 그냥 맡았던 시스템 1년 정도의 기간 동안 계속 고쳤던 얘기를 해보고 싶어서 글을 쓴다.
`지속적인 개선` 에 해당하는 단어가 뭐가 있을까 고민했는데 린 시스템에서도 그렇고 그냥 `Kaizen` 일본 단어 그대로 가져다 쓰길래 제목으로 써봤다.

## 메모리 누수 개선 1차

내가 담당하던 시스템은 응답 속도 이슈 때문에, 데이터를 서버 메모리에 직접 적재하고 떠야하는 종류의 시스템이었다.
다만 문제가 해당 서버가 오래 떠 있으면 떠 있을수록 계속 메모리 사용량이 증가하다가 결국은 90% 까지 사용량이 차버린다는 점이었다.
물론 어플리케이션이라는 게 메모리가 90퍼까지 찬다고, cpu 사용량이 100퍼가 된다고 무조건 죽냐 뭐 그런 건 아닌데,
운영하는 입장에서는 여유분 없이 계속 사용량이 차 있다는 것 자체가 불안 요소였다.

그래서 24년 연말에 시간이 남았을 때, 메모리 누수 원인이 뭔지 찾고 고치는 데 시간을 할애했었다.

맨 처음에는 실제로 누수가 있는 지를 찾는 것부터 시작했었다.
사실 메모리 누수를 찾아보는 것도 처음이었고, 해당 시스템에 대해서 그렇게까지 잘 알지도 못 했다.
그래서 챗쥐피티 도움도 받고 의심가는 것들 좀 뒤져보고 있었는데,
종국에 내가 옆에서 이것저것 뒤지는 걸 사수 분이 보시더니, 같이 찾기 시작해서 사수 분이 원인을 찾아내셨다.

원인은 메모리에 떠있어야하는 데이터 갱신을 위해서 서버에서 주기적으로 돌던 태스크 때문이었다.

서버에서 태스크가 도는 것만으로 메모리 누수가 있을 이유가 뭘까를 고민해보고 다른 라이브러리로 대체해보고 하다보니,
원인이 밝혀지게 됐다.

해당 코드는 쓰레드가 태스크를 실행하고 신규 쓰레드를 생성하여 다음 번 태스크를 실행하는 방식이었는데,
이 말인 즉슨 지속해서 쓰레드가 생성되기만 하고 회수되지는 않는다는 뜻이었다.
기존 쓰레드에 있던 자원은 처리가 끝났음에도 불구하고 다음 쓰레드에서 볼 수 있고 하다보니 제대로 회수가 되지 않았던 것이다.

원인을 알고 나서는 고치는 건 금방 했다.
기존처럼 매번 신규 쓰레드를 만드는 게 아니라 태스크를 실행할 쓰레드 하나만 생성해서 해당 쓰레드에서 태스크가 계속 실행되도록 만든 것이다.

나중에 안 거긴 한데 이 방법도 문제가 있긴 했다.
쓰레드를 계속 새로 생성하는 방식은 신규 쓰레드에서 계속 새로 실행하다보니까 한 두 번 실패해도 쓰레드 생성만 성공하면 나름 강건했는데,
하나의 쓰레드에서 실행하는 방법에서 예외처리 한 번 삐끗했더니 해당 서버에서는 갱신이 아예 중단이 되어버렸다.

해당 서버의 데이터가 자주 갱신되는 편도 아니었고, 신규 피쳐 요청이 계속 들어오는 서비스였어서,
좋든 싫든 서버가 1~2주마다 계속 새로 떴어서 위의 이슈가 메이저 이슈는 아니긴 했는데,
나중에 거진 2달간 업데이트가 없었던 기간에 데이터 갱신이 안 되는 것이 확인되고 나서 알게 되었다.

## 메모리 누수 개선 2차 및 인프라 최적화

25년에 내가 있던 직장은 리소스 감축에 진심이었다. 
사실 내가 담당하는 부분에서 쓰는 비용은 그렇게 큰게 아니었어서, 그 흐름에 꼭 올라타야 했느냐라고 하면 그건 아니긴 했는데,
그럼에도 불구하고 개인적으로 나는 비용 줄이는 작업 자체를 좋아했던 편이라 지속적으로 서버 개편을 하고자 했다.
이건 마인드의 차이이긴 한데, 비용 줄이는 작업하고 나면 월급값은 했다는 기분을 좀 즐길 수가 있다.
새 기능 개발해서 얼마 벌었는지는 체감하기 어렵지만 기존 기능 그대로 두고 얼마 아꼈는지는 숫자로 찍히기 때문이다.

일단 메모리 누수를 고친 것까지는 좋았지만, 이번에는 그냥 단순하게 데이터가 늘어났다.
그래서 순순하게 쓰는 데이터 자체가 많아지자 다시금 메모리 이슈가 붉어졌다.

이제와서 다시금 서버 구조를 봤을 때 불필요하게 많은 메모리를 쓰고 있는 곳을 뒤지기 시작했다.
문제는 프로세스풀을 사용하는 부분이었다.

기존에 이 서비스를 담당했던 분들은 서버 개발자가 아니였고, 지금 쓰는 언어를 쓰시던 분들이 아니었고, 기능의 구현도 현재와는 달랐다.
그런 시절에 개발되었던 프로세스풀을 사용한 병렬처리가 그대로 레거시로 남아있었던 것이다.

문제는 해당 서비스는 외부 서비스 호출 위주의 IO 중심 기능과 서버 내 자체 연산 기능 위주의 CPU 중심 기능이 뒤섞인 서버였고,
인프라는 vcpu 코어를 하나만 제공하고 있었다.
그렇다보니 사실 기존 방식으로는 딱히 성능 향상도 없었고 복잡하기만 했다.

내가 봤을 때 이 서버는 아무리 봐도 프로세스풀로 처리해야할 연산은 하나뿐이었고, 나머지는 쓰레드풀로 한다한들 차이가 없었다.
그리고 프로세스풀 쓰던 걸 쓰레드풀로 바꾸는 것만해도 메모리 사용량을 꽤나 줄일 수 있을 것이라 생각했다.

그래서 이런저런 상황 고려해서 아래처럼 변경을 했다.
- cpu 중심 기능만을 위한 별도의 프로세스 하나만 유지
- io 중심 기능은 모두 메인 프로세스에서 일어나도록 하되, io 성능은 유지될 수 있도록 쓰레드풀 숫자 튜닝
- 메인 프로세스, 별도 프로세스 이렇게 둘을 위해 컨테이너 하나 당 2vcpu 할당

결과적으로 성능에는 크게 영향을 안 주면서 메모리 사용량은 줄일 수 있었다.

이것도 하다가 나중에 문제를 발견한게,
예전에는 각 프로세스마다 커넥션을 다 따로 관리했어서 커넥션 풀을 작은 값을 줘도 별 상관이 없었는데,
쓰레드풀로 바꿨더니 커넥션을 공유하게 되어서 트래픽이 많이 몰리면 커넥션 풀 부족으로 대기가 발생했었다.

커넥션 풀 값 조금만 조정해줘도 해결되는 이슈긴 했는데,
대규모 변경하면 신경 쓸 게 온갖 곳에 있다는 걸 체감하게 된 경험이었다.

## 메인 쓰레드 블락킹 이슈

메모리 누수 이슈가 어느정도 해결이 되니까,
이제 응답시간에서 알람이 간간히 들려오기 시작했다.

이게 어쩌다 한 번이 아니라 주기적으로 트래픽이 좀 들어오는 시간대가 되면 어김없이 밀리길래,
최근에 변경된 게 뭐가 있었지 찾아봤더니,
최근 추가된 기능 관련 데이터가 3배 이상으로 늘어나 있었다.

해당 기능은 무거운 연산이 아니라고 생각했어서 안일하게 메인 프로세스에서 실행하고 있었는데,
메인 프로세스에서 실행되다보니 메인 쓰레드를 막아서 이벤트루프에서 다른 처리를 못 해서 그대로 해당 작업 동안의 모든 레이턴시가 나란히 밀렸던 것이다.

그래서 해당 기능도 별도의 프로세스로 연산을 빼는 것으로 해결했다.

## 중계서버 OOM 이슈 개선

이런저런 개선이 되고 있는 와중에, 서비스 서버에 데이터를 제공하는 중계 서버가 죽는 이슈가 보고되었다.

처음에는 원인을 몰라서 로그를 뒤지고 있었는데,
어찌됐건 서비스 서버가 스케일 아웃할 때 중계 서버가 죽는다는 거는 확인이 돼서 현상을 재현할 수가 있었다.
재현해봤더니 죽은 컨테이너 로그에 fatal OOM 이라고 적혀있었다.

확인해보니 서비스 서버가 스케일 아웃할 때 중계 서버에 트래픽이 몰려서 그런 것도 있긴 했지만,
문제는 서비스 서버에서 쓰는 데이터가 워낙 큰데, 중계 서버가 모든 요청마다 해당 데이터를 다 메모리에 올리고 있었기 때문에 OOM이 터졌던 것이다.

결국 해결은 큰 데이터가 메모리에 모든 요청마다 올라가지 않도록 하고, 메모리에 올라갔다면 그 값을 어느정도의 기간동안에 재사용하는 것으로 마무리되었다.

이거 고치면서 특정 고비용 연산을 딜레이시켜서 한 번 연산하고 여러 군데에 활용할 수 있도록 하는 싱글 플라이트에 대해서 처음 알았다.
추가로 원래라면 서버에 상태 만드는 거 싫어해서 될 수 있는한 서버 메모리 캐시 쓰는 걸 선호하지 않는 편인데,
문자열 형태로 공용 캐시에 저장하려 했더니 공용 캐시를 사용하기 위해서 데이터를 파싱하는 과정 자체가 거대한 데이터를 메모리에 올리게 만들었다.
그래서 별 수 없이 응답에 그대로 사용할 수 있는 바이트형태로 로컬 서버 메모리에 올려서 해결했다.

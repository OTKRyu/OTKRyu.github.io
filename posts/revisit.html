<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Revisit</title><meta property="og:type" content="website"/><meta property="og:url" content="https://otkruy.github.io/posts/revisit"/><meta property="og:title" content="Revisit"/><meta property="og:description" content="otkryu,apache,mmap,polars,opensearch,shared_memory"/><meta property="og:site_name" content="OTKRyu&#x27;s blog"/><meta property="og:locale" content="ko_KR"/><meta name="next-head-count" content="9"/><link rel="preload" href="/_next/static/css/712db0cf82bf277b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/712db0cf82bf277b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f813a7991d8b5eee.js" defer=""></script><script src="/_next/static/chunks/pages/_app-898c436b230de4c1.js" defer=""></script><script src="/_next/static/chunks/996-446f66ef59abd107.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-b11a75725d7639b5.js" defer=""></script><script src="/_next/static/18_h865q1ZxlxQ-DqlIvu/_buildManifest.js" defer=""></script><script src="/_next/static/18_h865q1ZxlxQ-DqlIvu/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="bg-blue-400"><div class="p-5"><nav class="bg-white flex justify-between border rounded my-3"><div class="p-2">OTKRyu</div><div class="flex"><div class="p-2 hover:underline"><a href="/">Home</a></div><div class="p-2 hover:underline"><a href="/posts">Posts</a></div><div class="p-2 hover:underline"><a href="/tags">Tags</a></div><div class="p-2 hover:underline"><a href="/about">About</a></div></div></nav><main><article class="bg-white border rounded my-3 w-full h-full p-5"><h1 class="text-3xl my-3">Revisit</h1><div class="text-slate-500">Fri Jan 16 2026 23:06:32 GMT+0900 (Korean Standard Time)</div><hr class="my-3"/><div class="prose lg:prose-xl px-2 m-auto my-4 sm:my-16"><h1>Revisit</h1>
<p>연말에 조직 개편 이슈로 인해 진행하던 업무들이 스탑되어서,
남는 시간동안 일을 열심히 할 것도 아니고 해서, 제미나이랑 놀았다.</p>
<p>제미나이랑 놀면서 문득 든 생각이 기존에 내가 해결했던 이슈 중에 <code>확실히 이것이 원인이다!</code> 라고 못한채 종결한 이슈들이 좀 있는데,
제미나이한테 물어봤을 때는 무엇을 원인으로 꼽을까 혹은 실제로 그 때 이슈가 되었던 것은 무엇이었을까가 궁금해졌다.</p>
<p>그래서 그 때 당시에 상황을 어떻게 파악했고 현상이 무엇이 있었는지만 넘겨준 채 제미나이랑 얘기하다보니,
생각보다 그 때 당시에는 몰랐던 사항들이 많이 나와서 신기해서 포스팅한다.</p>
<h2>Apache enable MMAP</h2>
<p>일단 이 이슈는 해결은 했지만 사실 완전히 잘못 이해하고 있었던 것으로 보인다.
기존 포스트에 보면 thread safe 하지 않은 것 때문에 발생했다고 생각했는데,
이번에 제미나이랑 얘기하면서 찾아본 바로는 직접적인 원인은 아닌 것으로 보인다.</p>
<p>먼저 mmap 옵션에 대한 생각 자체가 틀렸었다.
mmap는 기존에 생각하던 것처럼 하드 디스크 내용을 전부 메모리에 올려버리는 옵션이 아니었다.
mmap는 하드 디스크영역에 대한 테이블을 메모리영역에 미리 생성해주는 옵션이었다.</p>
<p>이해하기로는 아래와 같다.
mmap off</p>
<ul>
<li>작동 방식 :  디스크 정보 접근 -> 메모리 계층별로 접근하다가 disk에 있다는 점 파악 -> 커널 모드 진입 -> 커널에서 디스크 읽어서 데이터 복사 -> 커널에서 준 데이터를 유저 프로세스가 복사 받아서 데이터 취득(메모리 복사 disk에서 커널로 한 번, 커널에서 프로세스 메모리로 한 번)</li>
<li>반복 : 이후에도 동일한 정보에 접근하더라도 위 과정을 진행하는 것은 동일, 다만 disk에서 읽은 정보를 커널에서 유지한다면 더 빨라질 여지 있음. 좋든 싫든 커널 모드는 들어가야 되고, 커널에서 유저 프로세스로의 메모리 복사는 존재</li>
</ul>
<p>mmap on</p>
<ul>
<li>메모리에 디스크 접근 정보 테이블 미리 생성 -> 접근 시에 disk에 있다는 점 파악 -> 커널 모드 진입 -> 커널 캐시 영역에 데이터 로드 ->  커널 캐시 영역을 apache 프로세스와 공유 -> 메모리 복사없이 캐시 영역에 접근하여 데이터 취득(메모리 복사 disk에서 커널로 한 번)</li>
<li>반복 : 동일한 데이터면 공유된 메모리 영역에 있으니 바로 읽을 수 있음. 커널 모드 진입도 없고, 메모리 복사도 없음</li>
</ul>
<p>그렇다보니 딱히 read만 한다는 가정하에는 thread 충돌이고 자시고 할 게 딱히 없어보인다.</p>
<p>그래서 내가 생각한 이유가 아닌 것 같아서 의심가는 거 있냐고 물었더니,
네트워크 파일 시스템 붙어있는 거였냐고 질문이 돌아왔다.</p>
<p>실제로 해당 서비스는 IDC에서 제공되고 있었는데,
4대의 서버에서 동일한 서비스를 제공했기 때문에 4대의 서버에 모두 데이터를 옮기는 대신 네트워크 파일 시스템을 붙여서 업로드는 한 번만 하고 4대의 서버에서 나눠쓰는 식이었다.</p>
<p>결국 원인은 apache가 살아있는 동안 mmap 옵션으로 인해 파일들이 있다고 캐시 영역에 테이블을 만들어놨는데,
apache가 눈치채지도 못하게 네트워크 파일 시스템에서 파일에 변동이 생기면,
추후에 변동이 생긴 파일에 대한 요청이 들어올 시 매핑 정보가 틀어지면서 접근을 정상적으로 하지 못하게 되자 에러로 core dump 남기고 죽는 거였다.</p>
<p>enableMMAP 옵션이 원인인 것은 맞았던 것으로 보이나,
이해하고 있는 세부사항은 완전히 틀렸었다.</p>
<p>어찌됐건 해당 옵션 끄고 운영하다가 nginx로 바꾸고 nginx에서는 파일 read 처리하는 방법 및 프로세스 및 쓰레드 에러 시 대응 방식이 바뀌면서 해결 이슈가 됐다.</p>
<h2>Opensearch blow experience</h2>
<p>이 것도 포스트를 쓸 때는 스왑 때문 아닐까라고 생각했었는데,
제미나이랑 얘기하다보니 원인이 다른 것으로 보였다.</p>
<p>일단 상황 자체는 개발용으로 스펙 부족한 서버에다가 4096차원의 벡터 올려놓고 벡터 연산 때리다가,
오픈서치가 뻗어버린 일이었다.</p>
<p>처음에는 기존에 사용하던 인스턴스보다 메모리 사양이 적었기 때문에 메모리에 인덱스 구조체를 제대로 올리지 못하고,
이를 사용하려다보니 부족한 인덱스 구조체를 disk에 내렸다가 올렸다가하며 연산을 하느라 느린 거 아닌가라고 생각했었다.</p>
<p>근데 이번에 얘기하다가 당시의 인덱스 구조체 옵션 및 데이터 양을 봤을 때,
당시의 서버 스펙으로도 해당 인덱스 구조체를 올리기에는 충분하다는 계산 결과가 나왔다.</p>
<p>그래서 예전 상황을 하나하나 짚어보니 새롭게 등장한 옵션들은 아래와 같았다.</p>
<ol>
<li>그냥 차원이 커서 계산하느라 정신이 없었다. 순수 연산만으로 cpu 100 도달</li>
<li>해당 인덱스를 생성하고 맨 처음 보낸 쿼리였으니, 오픈 서치 같이 캐시를 달궈야하는 데이터베이스에서는 연산도 벅찬데 disk에 있는 정보도 메모리에 올려야돼서 cpu 100 도달</li>
</ol>
<p>이 얘기를 듣고나니 당시 인스턴스 타입이 t 타입이여서,
기본적인 cpu 연산 파워 자체도 약했고, 버스트도 금방 소진됐다고 생각하니까 아다리가 맞는 부분이 있었다.</p>
<p>당시에는 급해서 찬찬히 실험해볼 시간은 없고 바로 인스턴스 용량 늘려서 해결했는데,
지금 와서 생각해보면 원인은 잘못 파악하고 있었던 것으로 보인다.
벡터 검색 구조체 메모리 용량 공식에다가 넣어봤으면 아마 메모리 이슈는 아니였다고 알 수 있었을 것 같은데,
이제 와서야 정정하는 꼴이 됐다.</p>
<h2>kaizen</h2>
<p>여기서 소개된 사례의 경우 완전히 다른 결과가 나왔다기보다는,
문답 주고 받다보니 결국 원래도 고려하던 옵션으로 수렴해가지고 적는다.</p>
<p>조건은 아래와 같았다.</p>
<ol>
<li>해당 서버는 응답시간 이슈가 있다. 100ms 넘어서 나가는 건수가 많으면 장애로 취급된다.</li>
<li>해당 서버는 메모리 상에서 대용량의 데이터를 갱신해야되는 경우가 있다.</li>
<li>해당 서버는 python으로 구현되어 있다.</li>
<li>위 이슈를 위해 현재로서는 하나의 서버에 2개의 프로세스를 사용하여 메인 프로세스에서 서버 응답 처리를, 서브 프로세스에서는 cpu bound 관련 태스크를 처리한다.</li>
<li>위 구조 때문에 프로세스 간에 데이터 공유가 이루어지고 있다.</li>
</ol>
<p>문답하면서 나간 조건이 몇 가지 더 있었는데, 정확히 기억이 안 나서 위처럼 적는다.</p>
<p>그러고 나서 제미나이 들려준 답변은 아래와 같았다.</p>
<ol>
<li>프로세스 간 데이터 공유는 응답시간 이슈가 있을 경우 좋은 옵션이 아니다. shared_memory와 같은 직접 공유 형식으로 바꾸길 권장</li>
<li>아니면 프로세스 별로 구분을 안 하고 싶다면 처리 연산 모두 thread로 하고 대신에 정말 오래 걸리는 연산은 다른 코어에서 돌릴 수 있게 GIL 회피할 수 있는 방안을 찾아라</li>
</ol>
<p>둘 다 고려해본 옵션이다보니까 이번 건 그나마 좀 제대로 파악하고 있었다란 느낌이 드는 사례였다.</p>
<p>다만 대용량의 데이터 갱신을 전제 조건으로 줬다보니까,
데이터 갱신을 작게 쪼개서 한다든가하는 조건에서 벗어나는 해결책은 잘 생각해내지는 못 하는 것으로 보였다.</p>
<h2>정리</h2>
<p>먼저 말하자면 제미나이가 한 말이 뭐 정답이 아닐 수도 있고,
실제로 제미나이가 제시한 것 외에도 다른 원인이 있었을 수도 있다.</p>
<p>다만 해결을 했음에도 불구하고 좀 찜찜하던 부분이 있었는데,
제미나이랑 얘기하면서 그런 찜찜했던 부분을 해소하는 다른 옵션들이 뭐가 있는지에 대해 생각해볼 수 있었다.</p>
<p>당시에는 어떤 지표를 봐야하는 지도 몰랐어서 놓친 힌트도 많았을텐데,
이제와서라도 기억을 토대로 조금 더 공부해볼 수 있어서 좋은 경험이었다.</p>
<p>다른 한편으로는 지금까지 어찌저찌 문제를 해결해오고 있긴 했는데,
속된 말로 진짜 <code>궁예질이나 하고 있었구나</code> 하는 생각이 좀 들었다.</p>
<p>어쨌건 결과적으로 해결한 거 보면 궁예질에 어느 정도는 소질이 있었는지도 모르겠다.</p>
</div></article></main><footer class="bg-white flex justify-center border rounded my-2"><div class="m-2"><a href="mailto:appn12@gmail.com"><img src="/images/email_icon.png" width="30" height="30"/></a></div><div class="m-2"><a href="https://github.com/OTKRyu"><img src="/images/GitHub-Mark-32px.png" width="30" height="30"/></a></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"id":"revisit","postData":{"id":"revisit","contentHtml":"\u003ch1\u003eRevisit\u003c/h1\u003e\n\u003cp\u003e연말에 조직 개편 이슈로 인해 진행하던 업무들이 스탑되어서,\n남는 시간동안 일을 열심히 할 것도 아니고 해서, 제미나이랑 놀았다.\u003c/p\u003e\n\u003cp\u003e제미나이랑 놀면서 문득 든 생각이 기존에 내가 해결했던 이슈 중에 \u003ccode\u003e확실히 이것이 원인이다!\u003c/code\u003e 라고 못한채 종결한 이슈들이 좀 있는데,\n제미나이한테 물어봤을 때는 무엇을 원인으로 꼽을까 혹은 실제로 그 때 이슈가 되었던 것은 무엇이었을까가 궁금해졌다.\u003c/p\u003e\n\u003cp\u003e그래서 그 때 당시에 상황을 어떻게 파악했고 현상이 무엇이 있었는지만 넘겨준 채 제미나이랑 얘기하다보니,\n생각보다 그 때 당시에는 몰랐던 사항들이 많이 나와서 신기해서 포스팅한다.\u003c/p\u003e\n\u003ch2\u003eApache enable MMAP\u003c/h2\u003e\n\u003cp\u003e일단 이 이슈는 해결은 했지만 사실 완전히 잘못 이해하고 있었던 것으로 보인다.\n기존 포스트에 보면 thread safe 하지 않은 것 때문에 발생했다고 생각했는데,\n이번에 제미나이랑 얘기하면서 찾아본 바로는 직접적인 원인은 아닌 것으로 보인다.\u003c/p\u003e\n\u003cp\u003e먼저 mmap 옵션에 대한 생각 자체가 틀렸었다.\nmmap는 기존에 생각하던 것처럼 하드 디스크 내용을 전부 메모리에 올려버리는 옵션이 아니었다.\nmmap는 하드 디스크영역에 대한 테이블을 메모리영역에 미리 생성해주는 옵션이었다.\u003c/p\u003e\n\u003cp\u003e이해하기로는 아래와 같다.\nmmap off\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e작동 방식 :  디스크 정보 접근 -\u003e 메모리 계층별로 접근하다가 disk에 있다는 점 파악 -\u003e 커널 모드 진입 -\u003e 커널에서 디스크 읽어서 데이터 복사 -\u003e 커널에서 준 데이터를 유저 프로세스가 복사 받아서 데이터 취득(메모리 복사 disk에서 커널로 한 번, 커널에서 프로세스 메모리로 한 번)\u003c/li\u003e\n\u003cli\u003e반복 : 이후에도 동일한 정보에 접근하더라도 위 과정을 진행하는 것은 동일, 다만 disk에서 읽은 정보를 커널에서 유지한다면 더 빨라질 여지 있음. 좋든 싫든 커널 모드는 들어가야 되고, 커널에서 유저 프로세스로의 메모리 복사는 존재\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003emmap on\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e메모리에 디스크 접근 정보 테이블 미리 생성 -\u003e 접근 시에 disk에 있다는 점 파악 -\u003e 커널 모드 진입 -\u003e 커널 캐시 영역에 데이터 로드 -\u003e  커널 캐시 영역을 apache 프로세스와 공유 -\u003e 메모리 복사없이 캐시 영역에 접근하여 데이터 취득(메모리 복사 disk에서 커널로 한 번)\u003c/li\u003e\n\u003cli\u003e반복 : 동일한 데이터면 공유된 메모리 영역에 있으니 바로 읽을 수 있음. 커널 모드 진입도 없고, 메모리 복사도 없음\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e그렇다보니 딱히 read만 한다는 가정하에는 thread 충돌이고 자시고 할 게 딱히 없어보인다.\u003c/p\u003e\n\u003cp\u003e그래서 내가 생각한 이유가 아닌 것 같아서 의심가는 거 있냐고 물었더니,\n네트워크 파일 시스템 붙어있는 거였냐고 질문이 돌아왔다.\u003c/p\u003e\n\u003cp\u003e실제로 해당 서비스는 IDC에서 제공되고 있었는데,\n4대의 서버에서 동일한 서비스를 제공했기 때문에 4대의 서버에 모두 데이터를 옮기는 대신 네트워크 파일 시스템을 붙여서 업로드는 한 번만 하고 4대의 서버에서 나눠쓰는 식이었다.\u003c/p\u003e\n\u003cp\u003e결국 원인은 apache가 살아있는 동안 mmap 옵션으로 인해 파일들이 있다고 캐시 영역에 테이블을 만들어놨는데,\napache가 눈치채지도 못하게 네트워크 파일 시스템에서 파일에 변동이 생기면,\n추후에 변동이 생긴 파일에 대한 요청이 들어올 시 매핑 정보가 틀어지면서 접근을 정상적으로 하지 못하게 되자 에러로 core dump 남기고 죽는 거였다.\u003c/p\u003e\n\u003cp\u003eenableMMAP 옵션이 원인인 것은 맞았던 것으로 보이나,\n이해하고 있는 세부사항은 완전히 틀렸었다.\u003c/p\u003e\n\u003cp\u003e어찌됐건 해당 옵션 끄고 운영하다가 nginx로 바꾸고 nginx에서는 파일 read 처리하는 방법 및 프로세스 및 쓰레드 에러 시 대응 방식이 바뀌면서 해결 이슈가 됐다.\u003c/p\u003e\n\u003ch2\u003eOpensearch blow experience\u003c/h2\u003e\n\u003cp\u003e이 것도 포스트를 쓸 때는 스왑 때문 아닐까라고 생각했었는데,\n제미나이랑 얘기하다보니 원인이 다른 것으로 보였다.\u003c/p\u003e\n\u003cp\u003e일단 상황 자체는 개발용으로 스펙 부족한 서버에다가 4096차원의 벡터 올려놓고 벡터 연산 때리다가,\n오픈서치가 뻗어버린 일이었다.\u003c/p\u003e\n\u003cp\u003e처음에는 기존에 사용하던 인스턴스보다 메모리 사양이 적었기 때문에 메모리에 인덱스 구조체를 제대로 올리지 못하고,\n이를 사용하려다보니 부족한 인덱스 구조체를 disk에 내렸다가 올렸다가하며 연산을 하느라 느린 거 아닌가라고 생각했었다.\u003c/p\u003e\n\u003cp\u003e근데 이번에 얘기하다가 당시의 인덱스 구조체 옵션 및 데이터 양을 봤을 때,\n당시의 서버 스펙으로도 해당 인덱스 구조체를 올리기에는 충분하다는 계산 결과가 나왔다.\u003c/p\u003e\n\u003cp\u003e그래서 예전 상황을 하나하나 짚어보니 새롭게 등장한 옵션들은 아래와 같았다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e그냥 차원이 커서 계산하느라 정신이 없었다. 순수 연산만으로 cpu 100 도달\u003c/li\u003e\n\u003cli\u003e해당 인덱스를 생성하고 맨 처음 보낸 쿼리였으니, 오픈 서치 같이 캐시를 달궈야하는 데이터베이스에서는 연산도 벅찬데 disk에 있는 정보도 메모리에 올려야돼서 cpu 100 도달\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e이 얘기를 듣고나니 당시 인스턴스 타입이 t 타입이여서,\n기본적인 cpu 연산 파워 자체도 약했고, 버스트도 금방 소진됐다고 생각하니까 아다리가 맞는 부분이 있었다.\u003c/p\u003e\n\u003cp\u003e당시에는 급해서 찬찬히 실험해볼 시간은 없고 바로 인스턴스 용량 늘려서 해결했는데,\n지금 와서 생각해보면 원인은 잘못 파악하고 있었던 것으로 보인다.\n벡터 검색 구조체 메모리 용량 공식에다가 넣어봤으면 아마 메모리 이슈는 아니였다고 알 수 있었을 것 같은데,\n이제 와서야 정정하는 꼴이 됐다.\u003c/p\u003e\n\u003ch2\u003ekaizen\u003c/h2\u003e\n\u003cp\u003e여기서 소개된 사례의 경우 완전히 다른 결과가 나왔다기보다는,\n문답 주고 받다보니 결국 원래도 고려하던 옵션으로 수렴해가지고 적는다.\u003c/p\u003e\n\u003cp\u003e조건은 아래와 같았다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e해당 서버는 응답시간 이슈가 있다. 100ms 넘어서 나가는 건수가 많으면 장애로 취급된다.\u003c/li\u003e\n\u003cli\u003e해당 서버는 메모리 상에서 대용량의 데이터를 갱신해야되는 경우가 있다.\u003c/li\u003e\n\u003cli\u003e해당 서버는 python으로 구현되어 있다.\u003c/li\u003e\n\u003cli\u003e위 이슈를 위해 현재로서는 하나의 서버에 2개의 프로세스를 사용하여 메인 프로세스에서 서버 응답 처리를, 서브 프로세스에서는 cpu bound 관련 태스크를 처리한다.\u003c/li\u003e\n\u003cli\u003e위 구조 때문에 프로세스 간에 데이터 공유가 이루어지고 있다.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e문답하면서 나간 조건이 몇 가지 더 있었는데, 정확히 기억이 안 나서 위처럼 적는다.\u003c/p\u003e\n\u003cp\u003e그러고 나서 제미나이 들려준 답변은 아래와 같았다.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e프로세스 간 데이터 공유는 응답시간 이슈가 있을 경우 좋은 옵션이 아니다. shared_memory와 같은 직접 공유 형식으로 바꾸길 권장\u003c/li\u003e\n\u003cli\u003e아니면 프로세스 별로 구분을 안 하고 싶다면 처리 연산 모두 thread로 하고 대신에 정말 오래 걸리는 연산은 다른 코어에서 돌릴 수 있게 GIL 회피할 수 있는 방안을 찾아라\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e둘 다 고려해본 옵션이다보니까 이번 건 그나마 좀 제대로 파악하고 있었다란 느낌이 드는 사례였다.\u003c/p\u003e\n\u003cp\u003e다만 대용량의 데이터 갱신을 전제 조건으로 줬다보니까,\n데이터 갱신을 작게 쪼개서 한다든가하는 조건에서 벗어나는 해결책은 잘 생각해내지는 못 하는 것으로 보였다.\u003c/p\u003e\n\u003ch2\u003e정리\u003c/h2\u003e\n\u003cp\u003e먼저 말하자면 제미나이가 한 말이 뭐 정답이 아닐 수도 있고,\n실제로 제미나이가 제시한 것 외에도 다른 원인이 있었을 수도 있다.\u003c/p\u003e\n\u003cp\u003e다만 해결을 했음에도 불구하고 좀 찜찜하던 부분이 있었는데,\n제미나이랑 얘기하면서 그런 찜찜했던 부분을 해소하는 다른 옵션들이 뭐가 있는지에 대해 생각해볼 수 있었다.\u003c/p\u003e\n\u003cp\u003e당시에는 어떤 지표를 봐야하는 지도 몰랐어서 놓친 힌트도 많았을텐데,\n이제와서라도 기억을 토대로 조금 더 공부해볼 수 있어서 좋은 경험이었다.\u003c/p\u003e\n\u003cp\u003e다른 한편으로는 지금까지 어찌저찌 문제를 해결해오고 있긴 했는데,\n속된 말로 진짜 \u003ccode\u003e궁예질이나 하고 있었구나\u003c/code\u003e 하는 생각이 좀 들었다.\u003c/p\u003e\n\u003cp\u003e어쨌건 결과적으로 해결한 거 보면 궁예질에 어느 정도는 소질이 있었는지도 모르겠다.\u003c/p\u003e\n","title":"Revisit","date":"2026-01-16","tags":["apache","mmap","polars","opensearch","shared_memory"]}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"revisit"},"buildId":"18_h865q1ZxlxQ-DqlIvu","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
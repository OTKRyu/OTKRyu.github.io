---
title: "Too many index cause performence issue in elasticsearch"
date: "2023-06-14"
tags:
  - elasticsearch
  - index policy
  - cluster
  - performence issue
---

# ElasticSearch

## 문제

필자가 다니고 있는 회사에서는 로그 검색을 위해 ELK 스택을 사용하고 있다.
대체로 2달 정도 로그를 보관하고 일 로그가 메트릭을 포함하여 30g정도씩 쌓이고 있는데,
문제는 로그 보관 기간이 길어질수록 검색 성능이 떨어진다는 점이었다.
클러스터로 구조 이전 후에 일주일 내외의 로그만 쌓였을 때는 크게 문제가 없었지만,
로그 보관량이 2달에 가까워지면 질수록 몇가지 kibana api에서 응답 시간이 크게 느려졌다.
대표적으로 metric dashboard와 indices management 페이지가 크게 느려졌다.
화면을 띄우는데만 거의 30초 가까이 걸렸다.

## 해결 과정

이런 성능 저하가 일어났다는 걸 알고 있음에도 바로 대처하지 못했던 이유들 중 하나는,
지표면에서 튀는 걸 보고 알아챌만한 경험이 없었다는 점이다.
해결이 되고 난 뒤에서나 알았는데 검색 시에 cpu 점유율이 급격하고 튀고 있었는데,
이게 중요한 지표인지와 이게 언제 튀었는지를 제대로 추적하지 않아서 하드웨어에 부담이 되고 있는 지를 파악하지 못했다.

맨 처음 시도했던 방법은 ELK 스택을 굴리면서 얻은 교훈 중 하나로,
일단 스펙을 키우면 안 되던게 될 수도 있다는 점이었다.
그래서 ELK 스택에 들어가는 여유 자원들을 전부 elasticsearch에 몰아줬고 서버 자원의 절반정도를 전부 투자했다.
맨 처음에는 서버가 터지던 수준의 트래픽도 스스로 감당하길래 어느정도 개선이 된 줄 알았으나,
응답 속도면에서 개선이 미미했기 때문에 그다지 의미가 있지는 않았다.

이 때 추가로 의심했던 것은 클러스터를 구성한 서버의 스펙이 모자란게 아닐까 싶었지만,
다른 테크 블로그에서 elasticsearch를 활용하는 수준을 보면 일 30g 수준의 데이터 2달치가 elasticsearch에 부담이 될 거라고는 생각이 들지 않았다.

그 다음으로 시도한 것은 segment merging이었다.
elasticsearch는 인덱스에 새로운 데이터가 계속 들어옴에 따라 segment를 추가하여 생성한다.
이렇게 하면 색인을 하는 동안에는 효율을 증가시킬 수 있지만,
신규 데이터가 더 이상 들어오지 않는다면 검색을 할 때마다 모든 segment에서 i/o가 발생하기 때문에,
검색 성능을 저하시킬 수가 있다.

그래서 index lifecycle policy에 forcemering 설정을 추가하고,
기존에 merging이 안 된 인덱스들에는 forcemerging을 일일히 해주었다.

다만 실망스럽게도 이로 인한 성능 개선은 미미했다.

결국 index 정책이 잘못되었단 생각이 들었으나,
지금까지 공부한 바로는 index 수가 많다는 이유로 성능 저하를 겪었다거나 하는 내용을 들어본 적이 없어서 의심만 하고 실행은 하지 못하고 있었다.

## 해결

### 힌트

어느 날 회사의 시니어 분께서 자기가 기술 블로그에서 본 글이 있는데,
현재 상황이랑 비슷한 것 같다면서 포스트 링크를 하나 보내주셨다.

[https://netflixtechblog.com/elasticsearch-indexing-strategy-in-asset-management-platform-amp-99332231e541]https://netflixtechblog.com/elasticsearch-indexing-strategy-in-asset-management-platform-amp-99332231e541

넷플릭스 테크 블로그의 포스트인데 실제로 현재 회사 상황과 비슷했다.
포스트 내용은 넷플릭스가 asset을 elasticsearch에 등록할 때마다 새로운 인덱스를 생성하여 쓰다가,
심각한 성능 저하를 마주하여 새로운 인덱스를 쓰는 대신 하나의 인덱스에 몰아넣고,
이를 rollover 시켜서 유지하는 방식으로 바꾼 뒤 성능을 개선했다 정도로 요약할 수 있다.
성능 저하의 이유는 인덱스별로 검색을 진행하다보니 수많은 인덱스별로 각각 i/o가 발생한다는 것과,
인덱스별로 크기가 다르다보니 병렬로 실행했을 때의 효율성 또한 떨어진다는 것이다.

현재 필자의 회사에서는 위 넷플릭스와 비슷하게 일자별로, 로그의 종류별로 모두 새로운 인덱스를 생성하여 관리하고 있었는데,
이러다보니 일마다 거의 30개의 새로운 인덱스가 생성되었다.
결국 2달치면 클러스터 설정까지 고려하면,
모든 클러스터 기준으로 7200개의 인덱스가 생성되었다.

### elasticsearch측의 경고

필자가 인덱스의 수에 대한 경고를 받지 않은 것은 아니었다.
elasticsearch 7.16 버전 기준 성능상의 이유로,
하나의 데이터 노드에 1000개 이상의 샤드를 담지 못하게 설정되어 있다.
이 경고를 진지하게 받아들여야했는데,
이 시기에는 성능 저하가 눈에 띄지 않아서 에러를 방지하기 위해 샤드 제한수를 올리는 식으로 대처했어서,
성능 저하가 일어날 때까지 제대로 대처하지를 못했다.

### 해결방법

결국 많은 인덱스가 성능 저하의 원인이라는 점을 깨닫고 선택할 수 있는 방법은 크게 두 가지가 있었다.
현재 일자별, 로그별로 인덱스를 생성하고 있는데, 둘 중 한 기준을 제거하는 것으로 인덱스 수를 줄이는 것이었다.

맨 처음에는 넷플릭스 블로그에 써져있는 것처럼 로그별로 데이터를 두고 일자 혹은 인덱스의 용량을 기준으로 rollover 시키려고 했다.
다만 이렇게 하려면 alias를 추가하고 이에 따른 신규 인덱스 추가, alias별 writable 인덱스의 설정등 해야할 것이 많았다.
rollover는 index를 기준으로 일어나는 게 아니라 alias를 기준으로 일어나는 것이기 때문이다.

그렇다보니 위 방법으로 조치하기에는 확장성도 떨어지고 유지보수도 어려울 것이라는 결론에 다다라,
일자별로 인덱스를 나누기로 했다.
로그의 종류별로 나누던 인덱스를 하나의 큰 인덱스 템플릿 안에 모으고,
하나의 인덱스 템플릿이 모든 종류의 로그에 대응할 수 있도록 설정했다.
그리고 일자별로 해당 거대 인덱스 하나만 생성하도록 변경한 후,
기존의 검색하던 방식 대신 kibana에서 쿼리를 공유하도록 변경하여 이전과 동일하게 검색할 수 있도록 조치했다.

이렇게 변경하고, 기존 방식으로 생성된 인덱스를 제거했더니,
현재 한달치정도의 로그가 새로 쌓였음에도 불구하고 기존에 20~30초 걸리던 kibana api도 1초~2초 내에 끊기게 되었다.
그 외에도 느려지고 난 후에 지속적으로 나던 warn 로그 또한 사라졌다.

## 느낀 점

이번 일을 겪으면서 가장 크게 느낀 것은 두 가지로,
일단 사용하고 있는 툴에서 경고를 하면 주의를 기울여 조치하라는 점과,
시니어가 괜히 시니어가 아니라는 점이었다.

elasticsearch 관련 성능 저하 관련해서는 필자도 검색을 안 해본 것은 아니다.
인덱스 수가 많은게 문제가 될 수 있는지까지도 검색을 해봤던 것으로 기억한다.
그럼에도 불구하고 정확한 원인이나 조치방법에 대해서는 제대로 찾지를 못했는데,
타 부서의 시니어분께서 거의 정확한 정보를 가져다주셨단 것이 신기했다.

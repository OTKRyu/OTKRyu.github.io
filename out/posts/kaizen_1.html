<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Kaizen 1</title><meta property="og:type" content="website"/><meta property="og:url" content="https://otkruy.github.io/posts/kaizen_1"/><meta property="og:title" content="Kaizen 1"/><meta property="og:description" content="otkryu,process,thread,server,continuous improvement,memory leak"/><meta property="og:site_name" content="OTKRyu&#x27;s blog"/><meta property="og:locale" content="ko_KR"/><meta name="next-head-count" content="9"/><link rel="preload" href="/_next/static/css/a0b6e7785065566d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a0b6e7785065566d.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f813a7991d8b5eee.js" defer=""></script><script src="/_next/static/chunks/pages/_app-898c436b230de4c1.js" defer=""></script><script src="/_next/static/chunks/996-446f66ef59abd107.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-310c95fc2645c470.js" defer=""></script><script src="/_next/static/zgW5eu1kLeLdIkFrPdBWY/_buildManifest.js" defer=""></script><script src="/_next/static/zgW5eu1kLeLdIkFrPdBWY/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="bg-blue-400"><div class="p-5"><nav class="bg-white flex justify-between border rounded my-3"><div class="p-2">OTKRyu</div><div class="flex"><div class="p-2 hover:underline"><a href="/">Home</a></div><div class="p-2 hover:underline"><a href="/posts">Posts</a></div><div class="p-2 hover:underline"><a href="/tags">Tags</a></div><div class="p-2 hover:underline"><a href="/about">About</a></div></div></nav><main><article class="bg-white border rounded my-3 w-full h-full p-5"><h1 class="text-3xl my-3">Kaizen 1</h1><div class="text-slate-500">Wed Dec 10 2025 22:28:12 GMT+0900 (Korean Standard Time)</div><hr class="my-3"/><div class="prose lg:prose-xl px-2 m-auto my-4 sm:my-16"><h1>kaizen 1</h1>
<p>별 다른 의도는 없고, 그냥 맡았던 시스템 1년 정도의 기간 동안 계속 고쳤던 얘기를 해보고 싶어서 글을 쓴다.
<code>지속적인 개선</code> 에 해당하는 단어가 뭐가 있을까 고민했는데 린 시스템에서도 그렇고 그냥 <code>Kaizen</code> 일본 단어 그대로 가져다 쓰길래 제목으로 써봤다.</p>
<h2>메모리 누수 개선 1차</h2>
<p>내가 담당하던 시스템은 응답 속도 이슈 때문에, 데이터를 서버 메모리에 직접 적재하고 떠야하는 종류의 시스템이었다.
다만 문제가 해당 서버가 오래 떠 있으면 떠 있을수록 계속 메모리 사용량이 증가하다가 결국은 90% 까지 사용량이 차버린다는 점이었다.
물론 어플리케이션이라는 게 메모리가 90퍼까지 찬다고, cpu 사용량이 100퍼가 된다고 무조건 죽냐 뭐 그런 건 아닌데,
운영하는 입장에서는 여유분 없이 계속 사용량이 차 있다는 것 자체가 불안 요소였다.</p>
<p>그래서 24년 연말에 시간이 남았을 때, 메모리 누수 원인이 뭔지 찾고 고치는 데 시간을 할애했었다.</p>
<p>맨 처음에는 실제로 누수가 있는 지를 찾는 것부터 시작했었다.
사실 메모리 누수를 찾아보는 것도 처음이었고, 해당 시스템에 대해서 그렇게까지 잘 알지도 못 했다.
그래서 챗쥐피티 도움도 받고 의심가는 것들 좀 뒤져보고 있었는데,
종국에 내가 옆에서 이것저것 뒤지는 걸 사수 분이 보시더니, 같이 찾기 시작해서 사수 분이 원인을 찾아내셨다.</p>
<p>원인은 메모리에 떠있어야하는 데이터 갱신을 위해서 서버에서 주기적으로 돌던 태스크 때문이었다.</p>
<p>서버에서 태스크가 도는 것만으로 메모리 누수가 있을 이유가 뭘까를 고민해보고 다른 라이브러리로 대체해보고 하다보니,
원인이 밝혀지게 됐다.</p>
<p>해당 코드는 쓰레드가 태스크를 실행하고 신규 쓰레드를 생성하여 다음 번 태스크를 실행하는 방식이었는데,
이 말인 즉슨 지속해서 쓰레드가 생성되기만 하고 회수되지는 않는다는 뜻이었다.
기존 쓰레드에 있던 자원은 처리가 끝났음에도 불구하고 다음 쓰레드에서 볼 수 있고 하다보니 제대로 회수가 되지 않았던 것이다.</p>
<p>원인을 알고 나서는 고치는 건 금방 했다.
기존처럼 매번 신규 쓰레드를 만드는 게 아니라 태스크를 실행할 쓰레드 하나만 생성해서 해당 쓰레드에서 태스크가 계속 실행되도록 만든 것이다.</p>
<p>나중에 안 거긴 한데 이 방법도 문제가 있긴 했다.
쓰레드를 계속 새로 생성하는 방식은 신규 쓰레드에서 계속 새로 실행하다보니까 한 두 번 실패해도 쓰레드 생성만 성공하면 나름 강건했는데,
하나의 쓰레드에서 실행하는 방법에서 예외처리 한 번 삐끗했더니 해당 서버에서는 갱신이 아예 중단이 되어버렸다.</p>
<p>해당 서버의 데이터가 자주 갱신되는 편도 아니었고, 신규 피쳐 요청이 계속 들어오는 서비스였어서,
좋든 싫든 서버가 1~2주마다 계속 새로 떴어서 위의 이슈가 메이저 이슈는 아니긴 했는데,
나중에 거진 2달간 업데이트가 없었던 기간에 데이터 갱신이 안 되는 것이 확인되고 나서 알게 되었다.</p>
<h2>메모리 누수 개선 2차 및 인프라 최적화</h2>
<p>25년에 내가 있던 직장은 리소스 감축에 진심이었다.
사실 내가 담당하는 부분에서 쓰는 비용은 그렇게 큰게 아니었어서, 그 흐름에 꼭 올라타야 했느냐라고 하면 그건 아니긴 했는데,
그럼에도 불구하고 개인적으로 나는 비용 줄이는 작업 자체를 좋아했던 편이라 지속적으로 서버 개편을 하고자 했다.
이건 마인드의 차이이긴 한데, 비용 줄이는 작업하고 나면 월급값은 했다는 기분을 좀 즐길 수가 있다.
새 기능 개발해서 얼마 벌었는지는 체감하기 어렵지만 기존 기능 그대로 두고 얼마 아꼈는지는 숫자로 찍히기 때문이다.</p>
<p>일단 메모리 누수를 고친 것까지는 좋았지만, 이번에는 그냥 단순하게 데이터가 늘어났다.
그래서 순순하게 쓰는 데이터 자체가 많아지자 다시금 메모리 이슈가 붉어졌다.</p>
<p>이제와서 다시금 서버 구조를 봤을 때 불필요하게 많은 메모리를 쓰고 있는 곳을 뒤지기 시작했다.
문제는 프로세스풀을 사용하는 부분이었다.</p>
<p>기존에 이 서비스를 담당했던 분들은 서버 개발자가 아니였고, 지금 쓰는 언어를 쓰시던 분들이 아니었고, 기능의 구현도 현재와는 달랐다.
그런 시절에 개발되었던 프로세스풀을 사용한 병렬처리가 그대로 레거시로 남아있었던 것이다.</p>
<p>문제는 해당 서비스는 외부 서비스 호출 위주의 IO 중심 기능과 서버 내 자체 연산 기능 위주의 CPU 중심 기능이 뒤섞인 서버였고,
인프라는 vcpu 코어를 하나만 제공하고 있었다.
그렇다보니 사실 기존 방식으로는 딱히 성능 향상도 없었고 복잡하기만 했다.</p>
<p>내가 봤을 때 이 서버는 아무리 봐도 프로세스풀로 처리해야할 연산은 하나뿐이었고, 나머지는 쓰레드풀로 한다한들 차이가 없었다.
그리고 프로세스풀 쓰던 걸 쓰레드풀로 바꾸는 것만해도 메모리 사용량을 꽤나 줄일 수 있을 것이라 생각했다.</p>
<p>그래서 이런저런 상황 고려해서 아래처럼 변경을 했다.</p>
<ul>
<li>cpu 중심 기능만을 위한 별도의 프로세스 하나만 유지</li>
<li>io 중심 기능은 모두 메인 프로세스에서 일어나도록 하되, io 성능은 유지될 수 있도록 쓰레드풀 숫자 튜닝</li>
<li>메인 프로세스, 별도 프로세스 이렇게 둘을 위해 컨테이너 하나 당 2vcpu 할당</li>
</ul>
<p>결과적으로 성능에는 크게 영향을 안 주면서 메모리 사용량은 줄일 수 있었다.</p>
<p>이것도 하다가 나중에 문제를 발견한게,
예전에는 각 프로세스마다 커넥션을 다 따로 관리했어서 커넥션 풀을 작은 값을 줘도 별 상관이 없었는데,
쓰레드풀로 바꿨더니 커넥션을 공유하게 되어서 트래픽이 많이 몰리면 커넥션 풀 부족으로 대기가 발생했었다.</p>
<p>커넥션 풀 값 조금만 조정해줘도 해결되는 이슈긴 했는데,
대규모 변경하면 신경 쓸 게 온갖 곳에 있다는 걸 체감하게 된 경험이었다.</p>
<h2>메인 쓰레드 블락킹 이슈</h2>
<p>메모리 누수 이슈가 어느정도 해결이 되니까,
이제 응답시간에서 알람이 간간히 들려오기 시작했다.</p>
<p>이게 어쩌다 한 번이 아니라 주기적으로 트래픽이 좀 들어오는 시간대가 되면 어김없이 밀리길래,
최근에 변경된 게 뭐가 있었지 찾아봤더니,
최근 추가된 기능 관련 데이터가 3배 이상으로 늘어나 있었다.</p>
<p>해당 기능은 무거운 연산이 아니라고 생각했어서 안일하게 메인 프로세스에서 실행하고 있었는데,
메인 프로세스에서 실행되다보니 메인 쓰레드를 막아서 이벤트루프에서 다른 처리를 못 해서 그대로 해당 작업 동안의 모든 레이턴시가 나란히 밀렸던 것이다.</p>
<p>그래서 해당 기능도 별도의 프로세스로 연산을 빼는 것으로 해결했다.</p>
<h2>중계서버 OOM 이슈 개선</h2>
<p>이런저런 개선이 되고 있는 와중에, 서비스 서버에 데이터를 제공하는 중계 서버가 죽는 이슈가 보고되었다.</p>
<p>처음에는 원인을 몰라서 로그를 뒤지고 있었는데,
어찌됐건 서비스 서버가 스케일 아웃할 때 중계 서버가 죽는다는 거는 확인이 돼서 현상을 재현할 수가 있었다.
재현해봤더니 죽은 컨테이너 로그에 fatal OOM 이라고 적혀있었다.</p>
<p>확인해보니 서비스 서버가 스케일 아웃할 때 중계 서버에 트래픽이 몰려서 그런 것도 있긴 했지만,
문제는 서비스 서버에서 쓰는 데이터가 워낙 큰데, 중계 서버가 모든 요청마다 해당 데이터를 다 메모리에 올리고 있었기 때문에 OOM이 터졌던 것이다.</p>
<p>결국 해결은 큰 데이터가 메모리에 모든 요청마다 올라가지 않도록 하고, 메모리에 올라갔다면 그 값을 어느정도의 기간동안에 재사용하는 것으로 마무리되었다.</p>
<p>이거 고치면서 특정 고비용 연산을 딜레이시켜서 한 번 연산하고 여러 군데에 활용할 수 있도록 하는 싱글 플라이트에 대해서 처음 알았다.
추가로 원래라면 서버에 상태 만드는 거 싫어해서 될 수 있는한 서버 메모리 캐시 쓰는 걸 선호하지 않는 편인데,
문자열 형태로 공용 캐시에 저장하려 했더니 공용 캐시를 사용하기 위해서 데이터를 파싱하는 과정 자체가 거대한 데이터를 메모리에 올리게 만들었다.
그래서 별 수 없이 응답에 그대로 사용할 수 있는 바이트형태로 로컬 서버 메모리에 올려서 해결했다.</p>
</div></article></main><footer class="bg-white flex justify-center border rounded my-2"><div class="m-2"><a href="mailto:appn12@gmail.com"><img src="/images/email_icon.png" width="30" height="30"/></a></div><div class="m-2"><a href="https://github.com/OTKRyu"><img src="/images/GitHub-Mark-32px.png" width="30" height="30"/></a></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"id":"kaizen_1","postData":{"id":"kaizen_1","contentHtml":"\u003ch1\u003ekaizen 1\u003c/h1\u003e\n\u003cp\u003e별 다른 의도는 없고, 그냥 맡았던 시스템 1년 정도의 기간 동안 계속 고쳤던 얘기를 해보고 싶어서 글을 쓴다.\n\u003ccode\u003e지속적인 개선\u003c/code\u003e 에 해당하는 단어가 뭐가 있을까 고민했는데 린 시스템에서도 그렇고 그냥 \u003ccode\u003eKaizen\u003c/code\u003e 일본 단어 그대로 가져다 쓰길래 제목으로 써봤다.\u003c/p\u003e\n\u003ch2\u003e메모리 누수 개선 1차\u003c/h2\u003e\n\u003cp\u003e내가 담당하던 시스템은 응답 속도 이슈 때문에, 데이터를 서버 메모리에 직접 적재하고 떠야하는 종류의 시스템이었다.\n다만 문제가 해당 서버가 오래 떠 있으면 떠 있을수록 계속 메모리 사용량이 증가하다가 결국은 90% 까지 사용량이 차버린다는 점이었다.\n물론 어플리케이션이라는 게 메모리가 90퍼까지 찬다고, cpu 사용량이 100퍼가 된다고 무조건 죽냐 뭐 그런 건 아닌데,\n운영하는 입장에서는 여유분 없이 계속 사용량이 차 있다는 것 자체가 불안 요소였다.\u003c/p\u003e\n\u003cp\u003e그래서 24년 연말에 시간이 남았을 때, 메모리 누수 원인이 뭔지 찾고 고치는 데 시간을 할애했었다.\u003c/p\u003e\n\u003cp\u003e맨 처음에는 실제로 누수가 있는 지를 찾는 것부터 시작했었다.\n사실 메모리 누수를 찾아보는 것도 처음이었고, 해당 시스템에 대해서 그렇게까지 잘 알지도 못 했다.\n그래서 챗쥐피티 도움도 받고 의심가는 것들 좀 뒤져보고 있었는데,\n종국에 내가 옆에서 이것저것 뒤지는 걸 사수 분이 보시더니, 같이 찾기 시작해서 사수 분이 원인을 찾아내셨다.\u003c/p\u003e\n\u003cp\u003e원인은 메모리에 떠있어야하는 데이터 갱신을 위해서 서버에서 주기적으로 돌던 태스크 때문이었다.\u003c/p\u003e\n\u003cp\u003e서버에서 태스크가 도는 것만으로 메모리 누수가 있을 이유가 뭘까를 고민해보고 다른 라이브러리로 대체해보고 하다보니,\n원인이 밝혀지게 됐다.\u003c/p\u003e\n\u003cp\u003e해당 코드는 쓰레드가 태스크를 실행하고 신규 쓰레드를 생성하여 다음 번 태스크를 실행하는 방식이었는데,\n이 말인 즉슨 지속해서 쓰레드가 생성되기만 하고 회수되지는 않는다는 뜻이었다.\n기존 쓰레드에 있던 자원은 처리가 끝났음에도 불구하고 다음 쓰레드에서 볼 수 있고 하다보니 제대로 회수가 되지 않았던 것이다.\u003c/p\u003e\n\u003cp\u003e원인을 알고 나서는 고치는 건 금방 했다.\n기존처럼 매번 신규 쓰레드를 만드는 게 아니라 태스크를 실행할 쓰레드 하나만 생성해서 해당 쓰레드에서 태스크가 계속 실행되도록 만든 것이다.\u003c/p\u003e\n\u003cp\u003e나중에 안 거긴 한데 이 방법도 문제가 있긴 했다.\n쓰레드를 계속 새로 생성하는 방식은 신규 쓰레드에서 계속 새로 실행하다보니까 한 두 번 실패해도 쓰레드 생성만 성공하면 나름 강건했는데,\n하나의 쓰레드에서 실행하는 방법에서 예외처리 한 번 삐끗했더니 해당 서버에서는 갱신이 아예 중단이 되어버렸다.\u003c/p\u003e\n\u003cp\u003e해당 서버의 데이터가 자주 갱신되는 편도 아니었고, 신규 피쳐 요청이 계속 들어오는 서비스였어서,\n좋든 싫든 서버가 1~2주마다 계속 새로 떴어서 위의 이슈가 메이저 이슈는 아니긴 했는데,\n나중에 거진 2달간 업데이트가 없었던 기간에 데이터 갱신이 안 되는 것이 확인되고 나서 알게 되었다.\u003c/p\u003e\n\u003ch2\u003e메모리 누수 개선 2차 및 인프라 최적화\u003c/h2\u003e\n\u003cp\u003e25년에 내가 있던 직장은 리소스 감축에 진심이었다.\n사실 내가 담당하는 부분에서 쓰는 비용은 그렇게 큰게 아니었어서, 그 흐름에 꼭 올라타야 했느냐라고 하면 그건 아니긴 했는데,\n그럼에도 불구하고 개인적으로 나는 비용 줄이는 작업 자체를 좋아했던 편이라 지속적으로 서버 개편을 하고자 했다.\n이건 마인드의 차이이긴 한데, 비용 줄이는 작업하고 나면 월급값은 했다는 기분을 좀 즐길 수가 있다.\n새 기능 개발해서 얼마 벌었는지는 체감하기 어렵지만 기존 기능 그대로 두고 얼마 아꼈는지는 숫자로 찍히기 때문이다.\u003c/p\u003e\n\u003cp\u003e일단 메모리 누수를 고친 것까지는 좋았지만, 이번에는 그냥 단순하게 데이터가 늘어났다.\n그래서 순순하게 쓰는 데이터 자체가 많아지자 다시금 메모리 이슈가 붉어졌다.\u003c/p\u003e\n\u003cp\u003e이제와서 다시금 서버 구조를 봤을 때 불필요하게 많은 메모리를 쓰고 있는 곳을 뒤지기 시작했다.\n문제는 프로세스풀을 사용하는 부분이었다.\u003c/p\u003e\n\u003cp\u003e기존에 이 서비스를 담당했던 분들은 서버 개발자가 아니였고, 지금 쓰는 언어를 쓰시던 분들이 아니었고, 기능의 구현도 현재와는 달랐다.\n그런 시절에 개발되었던 프로세스풀을 사용한 병렬처리가 그대로 레거시로 남아있었던 것이다.\u003c/p\u003e\n\u003cp\u003e문제는 해당 서비스는 외부 서비스 호출 위주의 IO 중심 기능과 서버 내 자체 연산 기능 위주의 CPU 중심 기능이 뒤섞인 서버였고,\n인프라는 vcpu 코어를 하나만 제공하고 있었다.\n그렇다보니 사실 기존 방식으로는 딱히 성능 향상도 없었고 복잡하기만 했다.\u003c/p\u003e\n\u003cp\u003e내가 봤을 때 이 서버는 아무리 봐도 프로세스풀로 처리해야할 연산은 하나뿐이었고, 나머지는 쓰레드풀로 한다한들 차이가 없었다.\n그리고 프로세스풀 쓰던 걸 쓰레드풀로 바꾸는 것만해도 메모리 사용량을 꽤나 줄일 수 있을 것이라 생각했다.\u003c/p\u003e\n\u003cp\u003e그래서 이런저런 상황 고려해서 아래처럼 변경을 했다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecpu 중심 기능만을 위한 별도의 프로세스 하나만 유지\u003c/li\u003e\n\u003cli\u003eio 중심 기능은 모두 메인 프로세스에서 일어나도록 하되, io 성능은 유지될 수 있도록 쓰레드풀 숫자 튜닝\u003c/li\u003e\n\u003cli\u003e메인 프로세스, 별도 프로세스 이렇게 둘을 위해 컨테이너 하나 당 2vcpu 할당\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e결과적으로 성능에는 크게 영향을 안 주면서 메모리 사용량은 줄일 수 있었다.\u003c/p\u003e\n\u003cp\u003e이것도 하다가 나중에 문제를 발견한게,\n예전에는 각 프로세스마다 커넥션을 다 따로 관리했어서 커넥션 풀을 작은 값을 줘도 별 상관이 없었는데,\n쓰레드풀로 바꿨더니 커넥션을 공유하게 되어서 트래픽이 많이 몰리면 커넥션 풀 부족으로 대기가 발생했었다.\u003c/p\u003e\n\u003cp\u003e커넥션 풀 값 조금만 조정해줘도 해결되는 이슈긴 했는데,\n대규모 변경하면 신경 쓸 게 온갖 곳에 있다는 걸 체감하게 된 경험이었다.\u003c/p\u003e\n\u003ch2\u003e메인 쓰레드 블락킹 이슈\u003c/h2\u003e\n\u003cp\u003e메모리 누수 이슈가 어느정도 해결이 되니까,\n이제 응답시간에서 알람이 간간히 들려오기 시작했다.\u003c/p\u003e\n\u003cp\u003e이게 어쩌다 한 번이 아니라 주기적으로 트래픽이 좀 들어오는 시간대가 되면 어김없이 밀리길래,\n최근에 변경된 게 뭐가 있었지 찾아봤더니,\n최근 추가된 기능 관련 데이터가 3배 이상으로 늘어나 있었다.\u003c/p\u003e\n\u003cp\u003e해당 기능은 무거운 연산이 아니라고 생각했어서 안일하게 메인 프로세스에서 실행하고 있었는데,\n메인 프로세스에서 실행되다보니 메인 쓰레드를 막아서 이벤트루프에서 다른 처리를 못 해서 그대로 해당 작업 동안의 모든 레이턴시가 나란히 밀렸던 것이다.\u003c/p\u003e\n\u003cp\u003e그래서 해당 기능도 별도의 프로세스로 연산을 빼는 것으로 해결했다.\u003c/p\u003e\n\u003ch2\u003e중계서버 OOM 이슈 개선\u003c/h2\u003e\n\u003cp\u003e이런저런 개선이 되고 있는 와중에, 서비스 서버에 데이터를 제공하는 중계 서버가 죽는 이슈가 보고되었다.\u003c/p\u003e\n\u003cp\u003e처음에는 원인을 몰라서 로그를 뒤지고 있었는데,\n어찌됐건 서비스 서버가 스케일 아웃할 때 중계 서버가 죽는다는 거는 확인이 돼서 현상을 재현할 수가 있었다.\n재현해봤더니 죽은 컨테이너 로그에 fatal OOM 이라고 적혀있었다.\u003c/p\u003e\n\u003cp\u003e확인해보니 서비스 서버가 스케일 아웃할 때 중계 서버에 트래픽이 몰려서 그런 것도 있긴 했지만,\n문제는 서비스 서버에서 쓰는 데이터가 워낙 큰데, 중계 서버가 모든 요청마다 해당 데이터를 다 메모리에 올리고 있었기 때문에 OOM이 터졌던 것이다.\u003c/p\u003e\n\u003cp\u003e결국 해결은 큰 데이터가 메모리에 모든 요청마다 올라가지 않도록 하고, 메모리에 올라갔다면 그 값을 어느정도의 기간동안에 재사용하는 것으로 마무리되었다.\u003c/p\u003e\n\u003cp\u003e이거 고치면서 특정 고비용 연산을 딜레이시켜서 한 번 연산하고 여러 군데에 활용할 수 있도록 하는 싱글 플라이트에 대해서 처음 알았다.\n추가로 원래라면 서버에 상태 만드는 거 싫어해서 될 수 있는한 서버 메모리 캐시 쓰는 걸 선호하지 않는 편인데,\n문자열 형태로 공용 캐시에 저장하려 했더니 공용 캐시를 사용하기 위해서 데이터를 파싱하는 과정 자체가 거대한 데이터를 메모리에 올리게 만들었다.\n그래서 별 수 없이 응답에 그대로 사용할 수 있는 바이트형태로 로컬 서버 메모리에 올려서 해결했다.\u003c/p\u003e\n","title":"Kaizen 1","date":"2025-12-08","tags":["process","thread","server","continuous improvement","memory leak"]}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"kaizen_1"},"buildId":"zgW5eu1kLeLdIkFrPdBWY","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Logstash lack of memory</title><meta property="og:type" content="website"/><meta property="og:url" content="https://otkruy.github.io/posts/logstash_lack_of_memory"/><meta property="og:title" content="Logstash lack of memory"/><meta property="og:description" content="otkryu,elk stack,logstash,bug"/><meta property="og:site_name" content="OTKRyu&#x27;s blog"/><meta property="og:locale" content="ko_KR"/><meta name="next-head-count" content="9"/><link rel="preload" href="/_next/static/css/a0b6e7785065566d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/a0b6e7785065566d.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f813a7991d8b5eee.js" defer=""></script><script src="/_next/static/chunks/pages/_app-898c436b230de4c1.js" defer=""></script><script src="/_next/static/chunks/996-446f66ef59abd107.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bid%5D-310c95fc2645c470.js" defer=""></script><script src="/_next/static/o8u-Ud8cYdl3vO9tJP5y5/_buildManifest.js" defer=""></script><script src="/_next/static/o8u-Ud8cYdl3vO9tJP5y5/_ssgManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="bg-blue-400"><div class="p-5"><nav class="bg-white flex justify-between border rounded my-3"><div class="p-2">OTKRyu</div><div class="flex"><div class="p-2 hover:underline"><a href="/">Home</a></div><div class="p-2 hover:underline"><a href="/posts">Posts</a></div><div class="p-2 hover:underline"><a href="/tags">Tags</a></div><div class="p-2 hover:underline"><a href="/about">About</a></div></div></nav><main><article class="bg-white border rounded my-3 w-full h-full p-5"><h1 class="text-3xl my-3">Logstash lack of memory</h1><div class="text-slate-500">Thu Dec 11 2025 19:50:10 GMT+0900 (Korean Standard Time)</div><hr class="my-3"/><div class="prose lg:prose-xl px-2 m-auto my-4 sm:my-16"><h1>logstash lack of memory</h1>
<h2>logstash(ELK stack)</h2>
<p>먼저 무슨 일이 있었는지 기록하기 전에 ELK 스택에 대해 간략하게 설명하자면,
ELK 스택은 대체로 로그 및 모니터링 용도에 특화되어 있는 툴들을 모아놓은 것이다.</p>
<p>물론 각각 떼어내서 독립적으로 쓸 수도 있지만,
한 곳에서 개발하는 제품이다 보니 같이 쓰는 편이 손 쉽게 쓸 수 있고,
또 오픈 소스다보니 고급 기능을 쓰지 않는 이상 싸게 쓸 수 있다는 점도 있다.</p>
<p>다만 찾아보니 고급 기능도 꽤나 쓸만한 것들이 많아서,
돈이 있는 회사에 다니고 보안 상의 문제가 없으면,
그냥 돈 주고 클라우드 버전을 쓰는게 나아보인다.</p>
<p>어찌됐건 ELK 는 각각 elasticsearch, logstash, kibana로
각각의 툴들의 역할은 Nosql DB, log parser, visualizer이다</p>
<p>대부분 log를 logstash에 보내서 검색이나 보관에 용이한 형태로 변경하고,
elasticsearch에 색인해서 보관한 뒤에,
kibana에서 데이터를 조회하고 시각화한다.</p>
<h2>문제</h2>
<p>문제는 로그 누락이었다.</p>
<p>구축된 ELK에 추가적으로 로그를 수집하게 되어서,
신규 로그 수집 및 기존 로그에 영향이 없는지 모니터링을 하던 중이었다.</p>
<p>그런데 로그가 logstash 구동후 일마다 조금씩 조금씩 줄어들더니,
어느 순간에는 10%정도의 로그정도도 제대로 수집하지 못하기 시작했었다.</p>
<p>처음에는 ELK 스택에 지속적으로 작업이 있었기 때문에,
작업하면서 누락된 로그가 있었나보다라고 생각했다.</p>
<p>하지만 ELK 스택이 안정된 이후로도 이런 일이 지속적으로 일어났기 때문에,
뭔가 이상이 있다는 것을 알 수 있었다.</p>
<h2>과정</h2>
<p>해결하려고 하면서 뒤져본 순서는 아래와 같다.</p>
<p>beats(로그 수집기) => logstash(로그 파서) => elasticsearch(데이터베이스)</p>
<h3>beats 구간</h3>
<p>먼저 로그 수집기가 제대로 수집을 하고 있었는 지를 확인했다.</p>
<p>다만 로그 수집기가 잘못됐을 가능성은 처음부터 그렇게 높다고 생각하지 않았기도 하고,
실제로도 문제가 있지 않았다.(beats 툴들은 대체로 작동법이 간단하고, 설정이 잘못되지 않았다면 서버의 상태에 영향받을 정도로 무거운 툴도 아니었기 때문이다.)</p>
<p>결국 의심이 가는 것은 플로우상 단일 파이프라인으로 구축되어 있는 로그 수집기 => logstash 구간이거나,
logstash => elasticsearch 구간일거라고 생각했다.</p>
<h3>logstash 구간</h3>
<p>이 때 의심했던 것은 로그 수집기 => logstash 구간이었다.</p>
<p>ELK 구축과 관련하여 자료조사를 해왔는데,
현재 내가 다니는 회사에서 사용하는 정도의 수준으로는 현재 클러스터로도 차고 넘칠 정도였기 때문이다.</p>
<p>그만큼 elasticsearch와 elastcisearch가 구축되어 있는 서버 스펙은 충분하다고 판단했다.</p>
<p>그래서 결국 로그가 잘 도착했는지와 logstash 자체의 상태를 모니터링했다.</p>
<p>logstash 자체 로그를 봤을 때,
파싱이 안 되어서 누락되는 로그는 없었다.</p>
<p>그리고 elasticsearch에서 조회해봤을 때,
로그가 누락되고 있는거지 안 오고 있는게 아니었기 때문에,
결국 문제는 logstash 자체라고 생각했다.</p>
<p>신기했던 것은 시간이 지나면서 조금씩 로그가 누락되는 모양이었다.</p>
<p>그래서 원인이 로그 처리속도가 로그의 양을 따라가지 못해서 생긴다고 생각했고,
logstash의 metric을 찾아봤다.</p>
<p>cpu 사용량도 딱히 높지 않았고,
memory 사용량도 서버에 영향을 줄 정도로 높지 않았다.</p>
<p>결국 logstash도 metric 상으로는 문제가 있는지 잘 알 수가 없었다.</p>
<p>로그 처리속도는 결국 cpu의 영향을 받을 테니,
cpu가 정상인 시점에서 내 추론이 틀렸다고 생각했으나,
회사 내 다른 사람들에게 물어보니 memory 부족으로도 처리용량에 영향을 받을 수 있다는 얘기를 해주셨다.</p>
<p>그래서 이게 맞는 지는 모르겠지만,
일단 서버 스펙 상 남아도는 메모리라도 logstash에 추가로 할당을 해주었다.</p>
<h2>해결</h2>
<p>그렇게 설정을 변경해주고 며칠간 실제 서버에 남는 로그와 ELK 스택에서 조회되는 로그의 대조를 계속했다.</p>
<p>memory를 늘려준 것이 효과가 있었다.</p>
<p>다만 그 이유에 대해서는 잘 모르고 있다가,
위와 같은 일련의 과정을 겪으면서,
ELK 스택 자체에 대한 모니터링이 필요하단 생각을 하게 됐다.</p>
<p>그래서 ELK 스택 자체에 대한 모니터링을 구축하다가,
그제서야 원인을 알게 됐다.</p>
<p>logstash 모니터링 결과에 나온 logstash 사용 memory가 7g였다.</p>
<p>기존에 logstash에 할당된 한계 memory는 1g였다.</p>
<p>그래서 logstash에서 메모리를 풀로 쓰고 있는데도,
서버기준에서 봤을 때는 신경 쓸 수준이 아니라고 판단했던 거였다.</p>
<p>제대로 연산을 처리할 수 있는 환경이 아니었음에도 불구하고 말이다.</p>
<p>이에 대한 생각을 못 했던 이유가,
logstash가 하는 일이 설정대로 로그를 파싱하는 일이고,
memory보다는 cpu가 많이 필요한 일이라고 생각했기 때문이다.</p>
<p>그리고 memory가 커짐으로써 얻을 수 있는 것은 queue의 buffer가 커지는 효과뿐이며,
이로 인한 이득은 갑작스러운 대량의 로그가 발생했을 때,
logstash가 out of memory로 죽어버리는 일을 막는 효과뿐이라고 생각했었다.</p>
<p>실제로 위와 같은 일을 겪어본 적이 있기 때문에,
logstash가 안 죽는다면 그건 memory는 충분하다는 뜻으로 받아들이고 있었다.</p>
<p>하지만 연산량이 많았다면 충분한 memory도 유의미한 영향을 줄 정도로 중요했다.</p>
<p>만약 위에서 묘사한 일이 벌어지고 있는 사람이 있다면,
logstash의 memory를 확인해보면 도움이 될 수도 있다.</p>
<h2>참고</h2>
<p>일련의 일을 겪으면서 ELK 스택의 모니터링의 중요성을 느끼고,
ELK 스택을 구축해줄 수 있는 방법에 대해 찾아봤다.</p>
<p>대체로 xpack이라는 플러그인을 통해서 자동으로 모니터링을 구축할 수 있다고 했는데,
단일서버 ELK 스택을 모두 docker로 돌릴 경우에 어떻게 하는 지에 대해서는 못 찾았다.</p>
<p>대신에 kibana에서 설정할 수 있는 stack monitoring 중에 self monitoring이라는 기능에 대해 알게 됐다.</p>
<p>간이로 이미 돌아가고 있는 ELK 스택 각각에 스스로의 metric을 kibana로 보내게 해서 monitoring하는 방식인 걸로 보인다.</p>
<p>물론 이 방식이 ELK 스택을 제작하는 회사에서 추천하는 방법도 아니고,
ELK 스택을 빡빡하게 쓰고 있는 곳의 경우 성능 저하를 일으킬 수도 있다마는,
나처럼 일단 제대로 구동되는지 확인이 급한 사람들을 써볼만 하다고 생각한다.</p>
</div></article></main><footer class="bg-white flex justify-center border rounded my-2"><div class="m-2"><a href="mailto:appn12@gmail.com"><img src="/images/email_icon.png" width="30" height="30"/></a></div><div class="m-2"><a href="https://github.com/OTKRyu"><img src="/images/GitHub-Mark-32px.png" width="30" height="30"/></a></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"id":"logstash_lack_of_memory","postData":{"id":"logstash_lack_of_memory","contentHtml":"\u003ch1\u003elogstash lack of memory\u003c/h1\u003e\n\u003ch2\u003elogstash(ELK stack)\u003c/h2\u003e\n\u003cp\u003e먼저 무슨 일이 있었는지 기록하기 전에 ELK 스택에 대해 간략하게 설명하자면,\nELK 스택은 대체로 로그 및 모니터링 용도에 특화되어 있는 툴들을 모아놓은 것이다.\u003c/p\u003e\n\u003cp\u003e물론 각각 떼어내서 독립적으로 쓸 수도 있지만,\n한 곳에서 개발하는 제품이다 보니 같이 쓰는 편이 손 쉽게 쓸 수 있고,\n또 오픈 소스다보니 고급 기능을 쓰지 않는 이상 싸게 쓸 수 있다는 점도 있다.\u003c/p\u003e\n\u003cp\u003e다만 찾아보니 고급 기능도 꽤나 쓸만한 것들이 많아서,\n돈이 있는 회사에 다니고 보안 상의 문제가 없으면,\n그냥 돈 주고 클라우드 버전을 쓰는게 나아보인다.\u003c/p\u003e\n\u003cp\u003e어찌됐건 ELK 는 각각 elasticsearch, logstash, kibana로\n각각의 툴들의 역할은 Nosql DB, log parser, visualizer이다\u003c/p\u003e\n\u003cp\u003e대부분 log를 logstash에 보내서 검색이나 보관에 용이한 형태로 변경하고,\nelasticsearch에 색인해서 보관한 뒤에,\nkibana에서 데이터를 조회하고 시각화한다.\u003c/p\u003e\n\u003ch2\u003e문제\u003c/h2\u003e\n\u003cp\u003e문제는 로그 누락이었다.\u003c/p\u003e\n\u003cp\u003e구축된 ELK에 추가적으로 로그를 수집하게 되어서,\n신규 로그 수집 및 기존 로그에 영향이 없는지 모니터링을 하던 중이었다.\u003c/p\u003e\n\u003cp\u003e그런데 로그가 logstash 구동후 일마다 조금씩 조금씩 줄어들더니,\n어느 순간에는 10%정도의 로그정도도 제대로 수집하지 못하기 시작했었다.\u003c/p\u003e\n\u003cp\u003e처음에는 ELK 스택에 지속적으로 작업이 있었기 때문에,\n작업하면서 누락된 로그가 있었나보다라고 생각했다.\u003c/p\u003e\n\u003cp\u003e하지만 ELK 스택이 안정된 이후로도 이런 일이 지속적으로 일어났기 때문에,\n뭔가 이상이 있다는 것을 알 수 있었다.\u003c/p\u003e\n\u003ch2\u003e과정\u003c/h2\u003e\n\u003cp\u003e해결하려고 하면서 뒤져본 순서는 아래와 같다.\u003c/p\u003e\n\u003cp\u003ebeats(로그 수집기) =\u003e logstash(로그 파서) =\u003e elasticsearch(데이터베이스)\u003c/p\u003e\n\u003ch3\u003ebeats 구간\u003c/h3\u003e\n\u003cp\u003e먼저 로그 수집기가 제대로 수집을 하고 있었는 지를 확인했다.\u003c/p\u003e\n\u003cp\u003e다만 로그 수집기가 잘못됐을 가능성은 처음부터 그렇게 높다고 생각하지 않았기도 하고,\n실제로도 문제가 있지 않았다.(beats 툴들은 대체로 작동법이 간단하고, 설정이 잘못되지 않았다면 서버의 상태에 영향받을 정도로 무거운 툴도 아니었기 때문이다.)\u003c/p\u003e\n\u003cp\u003e결국 의심이 가는 것은 플로우상 단일 파이프라인으로 구축되어 있는 로그 수집기 =\u003e logstash 구간이거나,\nlogstash =\u003e elasticsearch 구간일거라고 생각했다.\u003c/p\u003e\n\u003ch3\u003elogstash 구간\u003c/h3\u003e\n\u003cp\u003e이 때 의심했던 것은 로그 수집기 =\u003e logstash 구간이었다.\u003c/p\u003e\n\u003cp\u003eELK 구축과 관련하여 자료조사를 해왔는데,\n현재 내가 다니는 회사에서 사용하는 정도의 수준으로는 현재 클러스터로도 차고 넘칠 정도였기 때문이다.\u003c/p\u003e\n\u003cp\u003e그만큼 elasticsearch와 elastcisearch가 구축되어 있는 서버 스펙은 충분하다고 판단했다.\u003c/p\u003e\n\u003cp\u003e그래서 결국 로그가 잘 도착했는지와 logstash 자체의 상태를 모니터링했다.\u003c/p\u003e\n\u003cp\u003elogstash 자체 로그를 봤을 때,\n파싱이 안 되어서 누락되는 로그는 없었다.\u003c/p\u003e\n\u003cp\u003e그리고 elasticsearch에서 조회해봤을 때,\n로그가 누락되고 있는거지 안 오고 있는게 아니었기 때문에,\n결국 문제는 logstash 자체라고 생각했다.\u003c/p\u003e\n\u003cp\u003e신기했던 것은 시간이 지나면서 조금씩 로그가 누락되는 모양이었다.\u003c/p\u003e\n\u003cp\u003e그래서 원인이 로그 처리속도가 로그의 양을 따라가지 못해서 생긴다고 생각했고,\nlogstash의 metric을 찾아봤다.\u003c/p\u003e\n\u003cp\u003ecpu 사용량도 딱히 높지 않았고,\nmemory 사용량도 서버에 영향을 줄 정도로 높지 않았다.\u003c/p\u003e\n\u003cp\u003e결국 logstash도 metric 상으로는 문제가 있는지 잘 알 수가 없었다.\u003c/p\u003e\n\u003cp\u003e로그 처리속도는 결국 cpu의 영향을 받을 테니,\ncpu가 정상인 시점에서 내 추론이 틀렸다고 생각했으나,\n회사 내 다른 사람들에게 물어보니 memory 부족으로도 처리용량에 영향을 받을 수 있다는 얘기를 해주셨다.\u003c/p\u003e\n\u003cp\u003e그래서 이게 맞는 지는 모르겠지만,\n일단 서버 스펙 상 남아도는 메모리라도 logstash에 추가로 할당을 해주었다.\u003c/p\u003e\n\u003ch2\u003e해결\u003c/h2\u003e\n\u003cp\u003e그렇게 설정을 변경해주고 며칠간 실제 서버에 남는 로그와 ELK 스택에서 조회되는 로그의 대조를 계속했다.\u003c/p\u003e\n\u003cp\u003ememory를 늘려준 것이 효과가 있었다.\u003c/p\u003e\n\u003cp\u003e다만 그 이유에 대해서는 잘 모르고 있다가,\n위와 같은 일련의 과정을 겪으면서,\nELK 스택 자체에 대한 모니터링이 필요하단 생각을 하게 됐다.\u003c/p\u003e\n\u003cp\u003e그래서 ELK 스택 자체에 대한 모니터링을 구축하다가,\n그제서야 원인을 알게 됐다.\u003c/p\u003e\n\u003cp\u003elogstash 모니터링 결과에 나온 logstash 사용 memory가 7g였다.\u003c/p\u003e\n\u003cp\u003e기존에 logstash에 할당된 한계 memory는 1g였다.\u003c/p\u003e\n\u003cp\u003e그래서 logstash에서 메모리를 풀로 쓰고 있는데도,\n서버기준에서 봤을 때는 신경 쓸 수준이 아니라고 판단했던 거였다.\u003c/p\u003e\n\u003cp\u003e제대로 연산을 처리할 수 있는 환경이 아니었음에도 불구하고 말이다.\u003c/p\u003e\n\u003cp\u003e이에 대한 생각을 못 했던 이유가,\nlogstash가 하는 일이 설정대로 로그를 파싱하는 일이고,\nmemory보다는 cpu가 많이 필요한 일이라고 생각했기 때문이다.\u003c/p\u003e\n\u003cp\u003e그리고 memory가 커짐으로써 얻을 수 있는 것은 queue의 buffer가 커지는 효과뿐이며,\n이로 인한 이득은 갑작스러운 대량의 로그가 발생했을 때,\nlogstash가 out of memory로 죽어버리는 일을 막는 효과뿐이라고 생각했었다.\u003c/p\u003e\n\u003cp\u003e실제로 위와 같은 일을 겪어본 적이 있기 때문에,\nlogstash가 안 죽는다면 그건 memory는 충분하다는 뜻으로 받아들이고 있었다.\u003c/p\u003e\n\u003cp\u003e하지만 연산량이 많았다면 충분한 memory도 유의미한 영향을 줄 정도로 중요했다.\u003c/p\u003e\n\u003cp\u003e만약 위에서 묘사한 일이 벌어지고 있는 사람이 있다면,\nlogstash의 memory를 확인해보면 도움이 될 수도 있다.\u003c/p\u003e\n\u003ch2\u003e참고\u003c/h2\u003e\n\u003cp\u003e일련의 일을 겪으면서 ELK 스택의 모니터링의 중요성을 느끼고,\nELK 스택을 구축해줄 수 있는 방법에 대해 찾아봤다.\u003c/p\u003e\n\u003cp\u003e대체로 xpack이라는 플러그인을 통해서 자동으로 모니터링을 구축할 수 있다고 했는데,\n단일서버 ELK 스택을 모두 docker로 돌릴 경우에 어떻게 하는 지에 대해서는 못 찾았다.\u003c/p\u003e\n\u003cp\u003e대신에 kibana에서 설정할 수 있는 stack monitoring 중에 self monitoring이라는 기능에 대해 알게 됐다.\u003c/p\u003e\n\u003cp\u003e간이로 이미 돌아가고 있는 ELK 스택 각각에 스스로의 metric을 kibana로 보내게 해서 monitoring하는 방식인 걸로 보인다.\u003c/p\u003e\n\u003cp\u003e물론 이 방식이 ELK 스택을 제작하는 회사에서 추천하는 방법도 아니고,\nELK 스택을 빡빡하게 쓰고 있는 곳의 경우 성능 저하를 일으킬 수도 있다마는,\n나처럼 일단 제대로 구동되는지 확인이 급한 사람들을 써볼만 하다고 생각한다.\u003c/p\u003e\n","title":"Logstash lack of memory","date":"2023-02-04","tags":["elk stack","logstash","bug"]}},"__N_SSG":true},"page":"/posts/[id]","query":{"id":"logstash_lack_of_memory"},"buildId":"o8u-Ud8cYdl3vO9tJP5y5","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
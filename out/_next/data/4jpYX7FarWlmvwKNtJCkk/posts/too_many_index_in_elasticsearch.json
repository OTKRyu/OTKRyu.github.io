{"pageProps":{"id":"too_many_index_in_elasticsearch","postData":{"id":"too_many_index_in_elasticsearch","contentHtml":"<h1>ElasticSearch</h1>\n<h2>문제</h2>\n<p>필자가 다니고 있는 회사에서는 로그 검색을 위해 ELK 스택을 사용하고 있다.\n대체로 2달 정도 로그를 보관하고 일 로그가 메트릭을 포함하여 30g정도씩 쌓이고 있는데,\n문제는 로그 보관 기간이 길어질수록 검색 성능이 떨어진다는 점이었다.\n클러스터로 구조 이전 후에 일주일 내외의 로그만 쌓였을 때는 크게 문제가 없었지만,\n로그 보관량이 2달에 가까워지면 질수록 몇가지 kibana api에서 응답 시간이 크게 느려졌다.\n대표적으로 metric dashboard와 indices management 페이지가 크게 느려졌다.\n화면을 띄우는데만 거의 30초 가까이 걸렸다.</p>\n<h2>해결 과정</h2>\n<p>이런 성능 저하가 일어났다는 걸 알고 있음에도 바로 대처하지 못했던 이유들 중 하나는,\n지표면에서 튀는 걸 보고 알아챌만한 경험이 없었다는 점이다.\n해결이 되고 난 뒤에서나 알았는데 검색 시에 cpu 점유율이 급격하고 튀고 있었는데,\n이게 중요한 지표인지와 이게 언제 튀었는지를 제대로 추적하지 않아서 하드웨어에 부담이 되고 있는 지를 파악하지 못했다.</p>\n<p>맨 처음 시도했던 방법은 ELK 스택을 굴리면서 얻은 교훈 중 하나로,\n일단 스펙을 키우면 안 되던게 될 수도 있다는 점이었다.\n그래서 ELK 스택에 들어가는 여유 자원들을 전부 elasticsearch에 몰아줬고 서버 자원의 절반정도를 전부 투자했다.\n맨 처음에는 서버가 터지던 수준의 트래픽도 스스로 감당하길래 어느정도 개선이 된 줄 알았으나,\n응답 속도면에서 개선이 미미했기 때문에 그다지 의미가 있지는 않았다.</p>\n<p>이 때 추가로 의심했던 것은 클러스터를 구성한 서버의 스펙이 모자란게 아닐까 싶었지만,\n다른 테크 블로그에서 elasticsearch를 활용하는 수준을 보면 일 30g 수준의 데이터 2달치가 elasticsearch에 부담이 될 거라고는 생각이 들지 않았다.</p>\n<p>그 다음으로 시도한 것은 segment merging이었다.\nelasticsearch는 인덱스에 새로운 데이터가 계속 들어옴에 따라 segment를 추가하여 생성한다.\n이렇게 하면 색인을 하는 동안에는 효율을 증가시킬 수 있지만,\n신규 데이터가 더 이상 들어오지 않는다면 검색을 할 때마다 모든 segment에서 i/o가 발생하기 때문에,\n검색 성능을 저하시킬 수가 있다.</p>\n<p>그래서 index lifecycle policy에 forcemering 설정을 추가하고,\n기존에 merging이 안 된 인덱스들에는 forcemerging을 일일히 해주었다.</p>\n<p>다만 실망스럽게도 이로 인한 성능 개선은 미미했다.</p>\n<p>결국 index 정책이 잘못되었단 생각이 들었으나,\n지금까지 공부한 바로는 index 수가 많다는 이유로 성능 저하를 겪었다거나 하는 내용을 들어본 적이 없어서 의심만 하고 실행은 하지 못하고 있었다.</p>\n<h2>해결</h2>\n<h3>힌트</h3>\n<p>어느 날 회사의 시니어 분께서 자기가 기술 블로그에서 본 글이 있는데,\n현재 상황이랑 비슷한 것 같다면서 포스트 링크를 하나 보내주셨다.</p>\n<p>[https://netflixtechblog.com/elasticsearch-indexing-strategy-in-asset-management-platform-amp-99332231e541]https://netflixtechblog.com/elasticsearch-indexing-strategy-in-asset-management-platform-amp-99332231e541</p>\n<p>넷플릭스 테크 블로그의 포스트인데 실제로 현재 회사 상황과 비슷했다.\n포스트 내용은 넷플릭스가 asset을 elasticsearch에 등록할 때마다 새로운 인덱스를 생성하여 쓰다가,\n심각한 성능 저하를 마주하여 새로운 인덱스를 쓰는 대신 하나의 인덱스에 몰아넣고,\n이를 rollover 시켜서 유지하는 방식으로 바꾼 뒤 성능을 개선했다 정도로 요약할 수 있다.\n성능 저하의 이유는 인덱스별로 검색을 진행하다보니 수많은 인덱스별로 각각 i/o가 발생한다는 것과,\n인덱스별로 크기가 다르다보니 병렬로 실행했을 때의 효율성 또한 떨어진다는 것이다.</p>\n<p>현재 필자의 회사에서는 위 넷플릭스와 비슷하게 일자별로, 로그의 종류별로 모두 새로운 인덱스를 생성하여 관리하고 있었는데,\n이러다보니 일마다 거의 30개의 새로운 인덱스가 생성되었다.\n결국 2달치면 클러스터 설정까지 고려하면,\n모든 클러스터 기준으로 7200개의 인덱스가 생성되었다.</p>\n<h3>elasticsearch측의 경고</h3>\n<p>필자가 인덱스의 수에 대한 경고를 받지 않은 것은 아니었다.\nelasticsearch 7.16 버전 기준 성능상의 이유로,\n하나의 데이터 노드에 1000개 이상의 샤드를 담지 못하게 설정되어 있다.\n이 경고를 진지하게 받아들여야했는데,\n이 시기에는 성능 저하가 눈에 띄지 않아서 에러를 방지하기 위해 샤드 제한수를 올리는 식으로 대처했어서,\n성능 저하가 일어날 때까지 제대로 대처하지를 못했다.</p>\n<h3>해결방법</h3>\n<p>결국 많은 인덱스가 성능 저하의 원인이라는 점을 깨닫고 선택할 수 있는 방법은 크게 두 가지가 있었다.\n현재 일자별, 로그별로 인덱스를 생성하고 있는데, 둘 중 한 기준을 제거하는 것으로 인덱스 수를 줄이는 것이었다.</p>\n<p>맨 처음에는 넷플릭스 블로그에 써져있는 것처럼 로그별로 데이터를 두고 일자 혹은 인덱스의 용량을 기준으로 rollover 시키려고 했다.\n다만 이렇게 하려면 alias를 추가하고 이에 따른 신규 인덱스 추가, alias별 writable 인덱스의 설정등 해야할 것이 많았다.\nrollover는 index를 기준으로 일어나는 게 아니라 alias를 기준으로 일어나는 것이기 때문이다.</p>\n<p>그렇다보니 위 방법으로 조치하기에는 확장성도 떨어지고 유지보수도 어려울 것이라는 결론에 다다라,\n일자별로 인덱스를 나누기로 했다.\n로그의 종류별로 나누던 인덱스를 하나의 큰 인덱스 템플릿 안에 모으고,\n하나의 인덱스 템플릿이 모든 종류의 로그에 대응할 수 있도록 설정했다.\n그리고 일자별로 해당 거대 인덱스 하나만 생성하도록 변경한 후,\n기존의 검색하던 방식 대신 kibana에서 쿼리를 공유하도록 변경하여 이전과 동일하게 검색할 수 있도록 조치했다.</p>\n<p>이렇게 변경하고, 기존 방식으로 생성된 인덱스를 제거했더니,\n현재 한달치정도의 로그가 새로 쌓였음에도 불구하고 기존에 20~30초 걸리던 kibana api도 1초~2초 내에 끊기게 되었다.\n그 외에도 느려지고 난 후에 지속적으로 나던 warn 로그 또한 사라졌다.</p>\n<h2>느낀 점</h2>\n<p>이번 일을 겪으면서 가장 크게 느낀 것은 두 가지로,\n일단 사용하고 있는 툴에서 경고를 하면 주의를 기울여 조치하라는 점과,\n시니어가 괜히 시니어가 아니라는 점이었다.</p>\n<p>elasticsearch 관련 성능 저하 관련해서는 필자도 검색을 안 해본 것은 아니다.\n인덱스 수가 많은게 문제가 될 수 있는지까지도 검색을 해봤던 것으로 기억한다.\n그럼에도 불구하고 정확한 원인이나 조치방법에 대해서는 제대로 찾지를 못했는데,\n타 부서의 시니어분께서 거의 정확한 정보를 가져다주셨단 것이 신기했다.</p>\n","title":"Too many index cause performence issue in elasticsearch","date":"2023-06-14","tags":["elasticsearch","index policy","cluster","performence issue"]}},"__N_SSG":true}
{"pageProps":{"id":"logstash_lack_of_memory","postData":{"id":"logstash_lack_of_memory","contentHtml":"<h1>logstash lack of memory</h1>\n<h2>logstash(ELK stack)</h2>\n<p>먼저 무슨 일이 있었는지 기록하기 전에 ELK 스택에 대해 간략하게 설명하자면,\r\nELK 스택은 대체로 로그 및 모니터링 용도에 특화되어 있는 툴들을 모아놓은 것이다.</p>\n<p>물론 각각 떼어내서 독립적으로 쓸 수도 있지만,\r\n한 곳에서 개발하는 제품이다 보니 같이 쓰는 편이 손 쉽게 쓸 수 있고,\r\n또 오픈 소스다보니 고급 기능을 쓰지 않는 이상 싸게 쓸 수 있다는 점도 있다.</p>\n<p>다만 찾아보니 고급 기능도 꽤나 쓸만한 것들이 많아서,\r\n돈이 있는 회사에 다니고 보안 상의 문제가 없으면,\r\n그냥 돈 주고 클라우드 버전을 쓰는게 나아보인다.</p>\n<p>어찌됐건 ELK 는 각각 elasticsearch, logstash, kibana로\r\n각각의 툴들의 역할은 Nosql DB, log parser, visualizer이다</p>\n<p>대부분 log를 logstash에 보내서 검색이나 보관에 용이한 형태로 변경하고,\r\nelasticsearch에 색인해서 보관한 뒤에,\r\nkibana에서 데이터를 조회하고 시각화한다.</p>\n<h2>문제</h2>\n<p>문제는 로그 누락이었다.</p>\n<p>구축된 ELK에 추가적으로 로그를 수집하게 되어서,\r\n신규 로그 수집 및 기존 로그에 영향이 없는지 모니터링을 하던 중이었다.</p>\n<p>그런데 로그가 logstash 구동후 일마다 조금씩 조금씩 줄어들더니,\r\n어느 순간에는 10%정도의 로그정도도 제대로 수집하지 못하기 시작했었다.</p>\n<p>처음에는 ELK 스택에 지속적으로 작업이 있었기 때문에,\r\n작업하면서 누락된 로그가 있었나보다라고 생각했다.</p>\n<p>하지만 ELK 스택이 안정된 이후로도 이런 일이 지속적으로 일어났기 때문에,\r\n뭔가 이상이 있다는 것을 알 수 있었다.</p>\n<h2>과정</h2>\n<p>해결하려고 하면서 뒤져본 순서는 아래와 같다.</p>\n<p>beats(로그 수집기) => logstash(로그 파서) => elasticsearch(데이터베이스)</p>\n<h3>beats 구간</h3>\n<p>먼저 로그 수집기가 제대로 수집을 하고 있었는 지를 확인했다.</p>\n<p>다만 로그 수집기가 잘못됐을 가능성은 처음부터 그렇게 높다고 생각하지 않았기도 하고,\r\n실제로도 문제가 있지 않았다.(beats 툴들은 대체로 작동법이 간단하고, 설정이 잘못되지 않았다면 서버의 상태에 영향받을 정도로 무거운 툴도 아니었기 때문이다.)</p>\n<p>결국 의심이 가는 것은 플로우상 단일 파이프라인으로 구축되어 있는 로그 수집기 => logstash 구간이거나,\r\nlogstash => elasticsearch 구간일거라고 생각했다.</p>\n<h3>logstash 구간</h3>\n<p>이 때 의심했던 것은 로그 수집기 => logstash 구간이었다.</p>\n<p>ELK 구축과 관련하여 자료조사를 해왔는데,\r\n현재 내가 다니는 회사에서 사용하는 정도의 수준으로는 현재 클러스터로도 차고 넘칠 정도였기 때문이다.</p>\n<p>그만큼 elasticsearch와 elastcisearch가 구축되어 있는 서버 스펙은 충분하다고 판단했다.</p>\n<p>그래서 결국 로그가 잘 도착했는지와 logstash 자체의 상태를 모니터링했다.</p>\n<p>logstash 자체 로그를 봤을 때,\r\n파싱이 안 되어서 누락되는 로그는 없었다.</p>\n<p>그리고 elasticsearch에서 조회해봤을 때,\r\n로그가 누락되고 있는거지 안 오고 있는게 아니었기 때문에,\r\n결국 문제는 logstash 자체라고 생각했다.</p>\n<p>신기했던 것은 시간이 지나면서 조금씩 로그가 누락되는 모양이었다.</p>\n<p>그래서 원인이 로그 처리속도가 로그의 양을 따라가지 못해서 생긴다고 생각했고,\r\nlogstash의 metric을 찾아봤다.</p>\n<p>cpu 사용량도 딱히 높지 않았고,\r\nmemory 사용량도 서버에 영향을 줄 정도로 높지 않았다.</p>\n<p>결국 logstash도 metric 상으로는 문제가 있는지 잘 알 수가 없었다.</p>\n<p>로그 처리속도는 결국 cpu의 영향을 받을 테니,\r\ncpu가 정상인 시점에서 내 추론이 틀렸다고 생각했으나,\r\n회사 내 다른 사람들에게 물어보니 memory 부족으로도 처리용량에 영향을 받을 수 있다는 얘기를 해주셨다.</p>\n<p>그래서 이게 맞는 지는 모르겠지만,\r\n일단 서버 스펙 상 남아도는 메모리라도 logstash에 추가로 할당을 해주었다.</p>\n<h2>해결</h2>\n<p>그렇게 설정을 변경해주고 며칠간 실제 서버에 남는 로그와 ELK 스택에서 조회되는 로그의 대조를 계속했다.</p>\n<p>memory를 늘려준 것이 효과가 있었다.</p>\n<p>다만 그 이유에 대해서는 잘 모르고 있다가,\r\n위와 같은 일련의 과정을 겪으면서,\r\nELK 스택 자체에 대한 모니터링이 필요하단 생각을 하게 됐다.</p>\n<p>그래서 ELK 스택 자체에 대한 모니터링을 구축하다가,\r\n그제서야 원인을 알게 됐다.</p>\n<p>logstash 모니터링 결과에 나온 logstash 사용 memory가 7g였다.</p>\n<p>기존에 logstash에 할당된 한계 memory는 1g였다.</p>\n<p>그래서 logstash에서 메모리를 풀로 쓰고 있는데도,\r\n서버기준에서 봤을 때는 신경 쓸 수준이 아니라고 판단했던 거였다.</p>\n<p>제대로 연산을 처리할 수 있는 환경이 아니었음에도 불구하고 말이다.</p>\n<p>이에 대한 생각을 못 했던 이유가,\r\nlogstash가 하는 일이 설정대로 로그를 파싱하는 일이고,\r\nmemory보다는 cpu가 많이 필요한 일이라고 생각했기 때문이다.</p>\n<p>그리고 memory가 커짐으로써 얻을 수 있는 것은 queue의 buffer가 커지는 효과뿐이며,\r\n이로 인한 이득은 갑작스러운 대량의 로그가 발생했을 때,\r\nlogstash가 out of memory로 죽어버리는 일을 막는 효과뿐이라고 생각했었다.</p>\n<p>실제로 위와 같은 일을 겪어본 적이 있기 때문에,\r\nlogstash가 안 죽는다면 그건 memory는 충분하다는 뜻으로 받아들이고 있었다.</p>\n<p>하지만 연산량이 많았다면 충분한 memory도 유의미한 영향을 줄 정도로 중요했다.</p>\n<p>만약 위에서 묘사한 일이 벌어지고 있는 사람이 있다면,\r\nlogstash의 memory를 확인해보면 도움이 될 수도 있다.</p>\n<h2>참고</h2>\n<p>일련의 일을 겪으면서 ELK 스택의 모니터링의 중요성을 느끼고,\r\nELK 스택을 구축해줄 수 있는 방법에 대해 찾아봤다.</p>\n<p>대체로 xpack이라는 플러그인을 통해서 자동으로 모니터링을 구축할 수 있다고 했는데,\r\n단일서버 ELK 스택을 모두 docker로 돌릴 경우에 어떻게 하는 지에 대해서는 못 찾았다.</p>\n<p>대신에 kibana에서 설정할 수 있는 stack monitoring 중에 self monitoring이라는 기능에 대해 알게 됐다.</p>\n<p>간이로 이미 돌아가고 있는 ELK 스택 각각에 스스로의 metric을 kibana로 보내게 해서 monitoring하는 방식인 걸로 보인다.</p>\n<p>물론 이 방식이 ELK 스택을 제작하는 회사에서 추천하는 방법도 아니고,\r\nELK 스택을 빡빡하게 쓰고 있는 곳의 경우 성능 저하를 일으킬 수도 있다마는,\r\n나처럼 일단 제대로 구동되는지 확인이 급한 사람들을 써볼만 하다고 생각한다.</p>\n","title":"logstash lack of memory","date":"2023-02-04","tags":["elk stack","logstash","bug"]}},"__N_SSG":true}